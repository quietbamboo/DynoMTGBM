{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb1f96913c6945c4",
   "metadata": {},
   "source": [
    "# MTGBM"
   ]
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-28T14:53:31.353788Z",
     "start_time": "2025-03-28T14:26:37.490656Z"
    }
   },
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import random\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "from scipy.stats import pearsonr\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "import lightgbmmt as lgb\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "\n",
    "def return_mtgbm_x_y(df_data, tasks):\n",
    "    y = np.array(df_data[tasks].values)\n",
    "\n",
    "    auxiliary_data = []\n",
    "    if use_t_ph_embedding:\n",
    "        ph = df_data['ph'].values.reshape(-1, 1)\n",
    "        t = df_data['t'].values.reshape(-1, 1)\n",
    "        auxiliary_data.append(ph)\n",
    "        auxiliary_data.append(t)\n",
    "\n",
    "    if use_mw_logp:\n",
    "        mw = df_data['mw'].values.reshape(-1, 1)\n",
    "        logp = df_data['logp'].values.reshape(-1, 1)\n",
    "        auxiliary_data.append(mw)\n",
    "        auxiliary_data.append(logp)\n",
    "\n",
    "    protein_data = np.array(df_data[protein_column].tolist())\n",
    "    substrate_data = np.array(df_data[substrate_column].tolist())\n",
    "\n",
    "    x = np.hstack([protein_data, substrate_data] + auxiliary_data)\n",
    "    return x, y\n",
    "\n",
    "def return_scores(y_true, y_pred):\n",
    "    mask = y_true != fill_nan_value\n",
    "    y_true = y_true[mask]\n",
    "    y_pred = y_pred[mask]\n",
    "\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    pcc = pearsonr(y_true, y_pred)[0]\n",
    "\n",
    "    return rmse, mae, r2, pcc\n",
    "\n",
    "def print_scores(task_scores_dict):\n",
    "    for task_name in task_names:\n",
    "        print(f\"{task_name}\\t RMSE\\t MAE\\t R2\\t PCC\\t\")\n",
    "\n",
    "        task_val_scores = task_scores_dict[task_name]['val']\n",
    "        task_test_scores = task_scores_dict[task_name]['test']\n",
    "\n",
    "        val_metrics = [f\"{np.mean(task_val_scores[metric_name]):.4f}\\t\" for metric_name in\n",
    "                       score_names]\n",
    "        print(\"Val  \" + \" \".join(val_metrics))\n",
    "\n",
    "        test_metrics = [f\"{np.mean(task_test_scores[metric_name]):.4f}\\t\" for metric_name in\n",
    "                        score_names]\n",
    "        print(\"Test \" + \" \".join(test_metrics))\n",
    "        print()\n",
    "\n",
    "def self_kcatkm_rmse(preds, train_data):\n",
    "    labels = torch.tensor(train_data.get_label(), device=device)\n",
    "    preds = torch.tensor(preds, device=device)\n",
    "\n",
    "    # extract kcatkm values\n",
    "    labels = labels.view(num_tasks, -1).T[:, num_tasks - 1]\n",
    "    preds = preds.view(num_tasks, -1).T[:, num_tasks - 1]\n",
    "\n",
    "    # mask\n",
    "    valid_mask = labels != fill_nan_value\n",
    "    valid_labels = labels[valid_mask]\n",
    "    valid_preds = preds[valid_mask]\n",
    "\n",
    "    kcatkm_rmse = torch.sqrt(torch.mean((valid_labels - valid_preds) ** 2))\n",
    "\n",
    "    return 'rmse_kcatkm', kcatkm_rmse.item(), False\n",
    "\n",
    "\n",
    "def cal_grad(preds, train_data, ep=0):\n",
    "    labels = torch.tensor(train_data.get_label(), device=device)\n",
    "    preds = torch.tensor(preds, device=device)\n",
    "    labels = labels.view(num_tasks, -1).T\n",
    "    preds = preds.view(num_tasks, -1).T\n",
    "\n",
    "    # mask\n",
    "    valid_mask = labels != fill_nan_value\n",
    "    grad = torch.zeros_like(preds)\n",
    "    grad[valid_mask] = preds[valid_mask] - labels[valid_mask]\n",
    "\n",
    "    # sum\n",
    "    grad_final = grad.mean(dim=1)\n",
    "\n",
    "    # Hessian\n",
    "    grad_flattened = grad.T.flatten()\n",
    "    hess = torch.ones_like(grad_final)\n",
    "    hess2 = torch.ones_like(grad_flattened)\n",
    "\n",
    "    return grad_final.cpu().numpy(), hess.cpu().numpy(), grad_flattened.cpu().numpy(), hess2.cpu().numpy()\n",
    "\n",
    "\n",
    "# TODO Train model\n",
    "def train_mtgbm(params):\n",
    "    temp_params = deepcopy(params)\n",
    "    temp_params.update({\"verbosity\": -1, \"objective\": \"custom\", \"num_labels\": num_tasks, \"tree_learner\": 'serial2', \"num_threads\": num_threads})\n",
    "    num_iterations = temp_params.pop(\"num_iterations\")\n",
    "\n",
    "    task_scores_dict = {task_name: {'val': {name: [] for name in score_names}, 'test': {name: [] for name in score_names}} for task_name in task_names}\n",
    "\n",
    "    for fold_idx, (train_index, val_index) in enumerate(kf.split(train_val_x), start=1):\n",
    "        print(f\"Fold {fold_idx}\")\n",
    "        # split dataset\n",
    "        train_x, val_x = train_val_x[train_index], train_val_x[val_index]\n",
    "        train_y, val_y = train_val_y[train_index], train_val_y[val_index]\n",
    "\n",
    "        train_data = lgb.Dataset(train_x, label=train_y)\n",
    "\n",
    "        val_data = lgb.Dataset(val_x, label=val_y)\n",
    "\n",
    "        # get the best epoch number\n",
    "        evals_result_mt = {}\n",
    "        lgb.train(temp_params, train_data, num_iterations, valid_sets=[val_data],\n",
    "                  fobj=cal_grad, feval=self_kcatkm_rmse, verbose_eval=1000, evals_result=evals_result_mt,\n",
    "                  callbacks=[lgb.early_stopping(stopping_rounds=500)])\n",
    "        valid_records = evals_result_mt['valid_0']['rmse_kcatkm']\n",
    "        min_index = np.argmin(np.array(valid_records))\n",
    "        print(f\"valid_records min_index {min_index}\")\n",
    "\n",
    "        # train model for all scores of validation and test\n",
    "        train_data = lgb.Dataset(train_x, label=train_y)\n",
    "        val_data = lgb.Dataset(val_x, label=val_y)\n",
    "        evals_result_mt = {}\n",
    "        model = lgb.train(temp_params, train_data, min_index + 1, valid_sets=[val_data],\n",
    "                          fobj=cal_grad, feval=self_kcatkm_rmse, verbose_eval=1000,\n",
    "                          evals_result=evals_result_mt)\n",
    "        model.set_num_labels(num_tasks)\n",
    "\n",
    "        # validation predict\n",
    "        val_predicted = model.predict(val_x)\n",
    "        val_scores = {task_name: return_scores(val_y[:, idx], val_predicted[:, idx]) for idx, task_name in\n",
    "                      enumerate(task_names)}\n",
    "\n",
    "        # test predict\n",
    "        test_predicted = model.predict(test_x)\n",
    "        test_scores = {task_name: return_scores(test_y[:, idx], test_predicted[:, idx]) for idx, task_name in\n",
    "                       enumerate(task_names)}\n",
    "\n",
    "        # record\n",
    "        for task_name in task_names:\n",
    "            for score_idx, score_name in enumerate(score_names):\n",
    "                task_scores_dict[task_name]['val'][score_name].append(val_scores[task_name][score_idx])\n",
    "                task_scores_dict[task_name]['test'][score_name].append(test_scores[task_name][score_idx])\n",
    "        print(f\"Val  {val_scores} \\n Test {test_scores}\\n\")\n",
    "\n",
    "    print_scores(task_scores_dict)\n",
    "\n",
    "\n",
    "# init seed\n",
    "random_state = 66\n",
    "random.seed(random_state)\n",
    "np.random.seed(random_state)\n",
    "torch.manual_seed(random_state)\n",
    "torch.cuda.manual_seed(random_state)\n",
    "torch.cuda.manual_seed_all(random_state)\n",
    "\n",
    "# config\n",
    "protein_column,  substrate_column = 'prott5', 'molebert'\n",
    "input_model = 'mtgbm_km_kcat_kcatkm'\n",
    "dataset_path = f'{current_dir}/../../data_process/dataset/df_all_log_transformed.pkl'\n",
    "params_json_path = f'{current_dir}/../{input_model}_params.json'\n",
    "use_t_ph_embedding = True\n",
    "use_mw_logp = True\n",
    "num_threads = 32\n",
    "search_max_evals = 60\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Current device is {device}\")\n",
    "\n",
    "# input\n",
    "score_names = ['rmse', 'mae', 'r2', 'pcc']\n",
    "task_names = ['logkcat', 'logkcatkm']\n",
    "num_tasks = len(task_names)\n",
    "df_input = pd.read_pickle(dataset_path)\n",
    "fill_nan_value = -100\n",
    "df_input = df_input.fillna(fill_nan_value)\n",
    "\n",
    "# split dataset\n",
    "df_train_val, df_test = train_test_split(df_input, test_size=0.2, random_state=random_state)\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=random_state)\n",
    "train_val_x, train_val_y = return_mtgbm_x_y(df_train_val, task_names)\n",
    "test_x, test_y = return_mtgbm_x_y(df_test, task_names)\n",
    "\n",
    "if os.path.exists(params_json_path):\n",
    "    with open(params_json_path, 'r') as json_file:\n",
    "        best_params = json.load(json_file)\n",
    "\n",
    "    print('best_params:', best_params)\n",
    "    print('using kcat kcatkm resample')\n",
    "    train_mtgbm(best_params)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current device is cuda:0\n",
      "best_params: {'bagging_fraction': 0.729611058732434, 'feature_fraction': 0.6643005188332146, 'lambda_l1': 0.346846951564011, 'lambda_l2': 0.7149783548509333, 'learning_rate': 0.07838547411322133, 'max_bin': 95, 'max_depth': 9, 'min_data_in_leaf': 21, 'num_iterations': 3273, 'num_leaves': 2350}\n",
      "using kcat kcatkm resample\n",
      "Fold 1\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[1000]\tvalid_0's rmse_kcatkm: 2.70167\n",
      "[2000]\tvalid_0's rmse_kcatkm: 2.69625\n",
      "Early stopping, best iteration is:\n",
      "[1522]\tvalid_0's rmse_kcatkm: 2.69528\n",
      "valid_records min_index 1521\n",
      "[1000]\tvalid_0's rmse_kcatkm: 2.70167\n",
      "inner_predict 14892\n",
      "inner_predict 18614\n",
      "Val  {'logkcat': (2.114979108118392, 1.4327552897890838, 0.6321843949505954, 0.7958112269907817), 'logkcatkm': (2.695284117237703, 1.9164779643987764, 0.5922161440334032, 0.7698318627177706)} \n",
      " Test {'logkcat': (2.1939589507663366, 1.4918669155708784, 0.6270135813980355, 0.7925090115095166), 'logkcatkm': (2.6646397242070408, 1.9158805453811905, 0.5830581724103608, 0.7638878617682091)}\n",
      "\n",
      "Fold 2\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[1000]\tvalid_0's rmse_kcatkm: 2.58709\n",
      "Early stopping, best iteration is:\n",
      "[1035]\tvalid_0's rmse_kcatkm: 2.58663\n",
      "valid_records min_index 1034\n",
      "[1000]\tvalid_0's rmse_kcatkm: 2.58709\n",
      "inner_predict 14892\n",
      "inner_predict 18614\n",
      "Val  {'logkcat': (2.0883604834132425, 1.4404801798937645, 0.637695823070944, 0.7996264037442459), 'logkcatkm': (2.586629204690602, 1.8736095142733982, 0.6136117712568514, 0.7838726700432823)} \n",
      " Test {'logkcat': (2.2012617383376005, 1.5074216734404853, 0.6245264119330585, 0.7910199660311086), 'logkcatkm': (2.6813306601476317, 1.9161399184578525, 0.5778184810194917, 0.7603538079827667)}\n",
      "\n",
      "Fold 3\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[1000]\tvalid_0's rmse_kcatkm: 2.56204\n",
      "[2000]\tvalid_0's rmse_kcatkm: 2.54241\n",
      "[3000]\tvalid_0's rmse_kcatkm: 2.53967\n",
      "Early stopping, best iteration is:\n",
      "[2598]\tvalid_0's rmse_kcatkm: 2.53937\n",
      "valid_records min_index 2597\n",
      "[1000]\tvalid_0's rmse_kcatkm: 2.56204\n",
      "[2000]\tvalid_0's rmse_kcatkm: 2.54241\n",
      "inner_predict 14890\n",
      "inner_predict 18614\n",
      "Val  {'logkcat': (2.1414816514151274, 1.4454295681746363, 0.6288025329982141, 0.7950658675043282), 'logkcatkm': (2.5393654425457544, 1.8028701346834588, 0.618258122217729, 0.786949280341182)} \n",
      " Test {'logkcat': (2.158898041083998, 1.4535948578744595, 0.6388394650589075, 0.8001088395061294), 'logkcatkm': (2.647409631764568, 1.8617448459714678, 0.5884327975442256, 0.7687186374798256)}\n",
      "\n",
      "Fold 4\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[1000]\tvalid_0's rmse_kcatkm: 2.6424\n",
      "[2000]\tvalid_0's rmse_kcatkm: 2.6355\n",
      "Early stopping, best iteration is:\n",
      "[2094]\tvalid_0's rmse_kcatkm: 2.63496\n",
      "valid_records min_index 2093\n",
      "[1000]\tvalid_0's rmse_kcatkm: 2.6424\n",
      "[2000]\tvalid_0's rmse_kcatkm: 2.6355\n",
      "inner_predict 14890\n",
      "inner_predict 18614\n",
      "Val  {'logkcat': (2.0324603401399775, 1.4027302815399305, 0.6506854703201355, 0.8080832515022753), 'logkcatkm': (2.634961076500913, 1.8839526741906476, 0.5892664178327223, 0.768324363384274)} \n",
      " Test {'logkcat': (2.1654056503638777, 1.4726040101891191, 0.6366588767501191, 0.7986987810716459), 'logkcatkm': (2.6719179068659433, 1.8911647401698242, 0.5807773964631779, 0.763594092454235)}\n",
      "\n",
      "Fold 5\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[1000]\tvalid_0's rmse_kcatkm: 2.64459\n",
      "Early stopping, best iteration is:\n",
      "[1468]\tvalid_0's rmse_kcatkm: 2.64035\n",
      "valid_records min_index 1467\n",
      "[1000]\tvalid_0's rmse_kcatkm: 2.64459\n",
      "inner_predict 14890\n",
      "inner_predict 18614\n",
      "Val  {'logkcat': (2.099246315080549, 1.4321613014327321, 0.6533010478603076, 0.8097555453084245), 'logkcatkm': (2.6403466843984105, 1.871502090954791, 0.5995202145132417, 0.7756914360356948)} \n",
      " Test {'logkcat': (2.170124179135587, 1.4754401095333063, 0.6350736740286553, 0.7973674473727916), 'logkcatkm': (2.6418025650563304, 1.9007712709035947, 0.5901743043796578, 0.7683972463540802)}\n",
      "\n",
      "logkcat\t RMSE\t MAE\t R2\t PCC\t\n",
      "Val  2.0953\t 1.4307\t 0.6405\t 0.8017\t\n",
      "Test 2.1779\t 1.4802\t 0.6324\t 0.7959\t\n",
      "\n",
      "logkcatkm\t RMSE\t MAE\t R2\t PCC\t\n",
      "Val  2.6193\t 1.8697\t 0.6026\t 0.7769\t\n",
      "Test 2.6614\t 1.8971\t 0.5841\t 0.7650\t\n",
      "\n"
     ]
    }
   ],
   "execution_count": 1
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DynoMTGBM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "db0e6cebfd42dbe7de32cf1b0daf517db5c30eda4a99fad3eb7c5d8b4a7bde0f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
