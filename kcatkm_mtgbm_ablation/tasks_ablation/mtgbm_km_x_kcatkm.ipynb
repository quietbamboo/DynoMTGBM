{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb1f96913c6945c4",
   "metadata": {},
   "source": [
    "# MTGBM"
   ]
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-28T15:53:50.505804Z",
     "start_time": "2025-03-28T14:55:00.381748Z"
    }
   },
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import random\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "from scipy.stats import pearsonr\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "import lightgbmmt as lgb\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "\n",
    "def return_mtgbm_x_y(df_data, tasks):\n",
    "    y = np.array(df_data[tasks].values)\n",
    "\n",
    "    auxiliary_data = []\n",
    "    if use_t_ph_embedding:\n",
    "        ph = df_data['ph'].values.reshape(-1, 1)\n",
    "        t = df_data['t'].values.reshape(-1, 1)\n",
    "        auxiliary_data.append(ph)\n",
    "        auxiliary_data.append(t)\n",
    "\n",
    "    if use_mw_logp:\n",
    "        mw = df_data['mw'].values.reshape(-1, 1)\n",
    "        logp = df_data['logp'].values.reshape(-1, 1)\n",
    "        auxiliary_data.append(mw)\n",
    "        auxiliary_data.append(logp)\n",
    "\n",
    "    protein_data = np.array(df_data[protein_column].tolist())\n",
    "    substrate_data = np.array(df_data[substrate_column].tolist())\n",
    "\n",
    "    x = np.hstack([protein_data, substrate_data] + auxiliary_data)\n",
    "    return x, y\n",
    "\n",
    "def return_scores(y_true, y_pred):\n",
    "    mask = y_true != fill_nan_value\n",
    "    y_true = y_true[mask]\n",
    "    y_pred = y_pred[mask]\n",
    "\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    pcc = pearsonr(y_true, y_pred)[0]\n",
    "\n",
    "    return rmse, mae, r2, pcc\n",
    "\n",
    "def print_scores(task_scores_dict):\n",
    "    for task_name in task_names:\n",
    "        print(f\"{task_name}\\t RMSE\\t MAE\\t R2\\t PCC\\t\")\n",
    "\n",
    "        task_val_scores = task_scores_dict[task_name]['val']\n",
    "        task_test_scores = task_scores_dict[task_name]['test']\n",
    "\n",
    "        val_metrics = [f\"{np.mean(task_val_scores[metric_name]):.4f}\\t\" for metric_name in\n",
    "                       score_names]\n",
    "        print(\"Val  \" + \" \".join(val_metrics))\n",
    "\n",
    "        test_metrics = [f\"{np.mean(task_test_scores[metric_name]):.4f}\\t\" for metric_name in\n",
    "                        score_names]\n",
    "        print(\"Test \" + \" \".join(test_metrics))\n",
    "        print()\n",
    "\n",
    "def self_kcatkm_rmse(preds, train_data):\n",
    "    labels = torch.tensor(train_data.get_label(), device=device)\n",
    "    preds = torch.tensor(preds, device=device)\n",
    "\n",
    "    # extract kcatkm values\n",
    "    labels = labels.view(num_tasks, -1).T[:, num_tasks - 1]\n",
    "    preds = preds.view(num_tasks, -1).T[:, num_tasks - 1]\n",
    "\n",
    "    # mask\n",
    "    valid_mask = labels != fill_nan_value\n",
    "    valid_labels = labels[valid_mask]\n",
    "    valid_preds = preds[valid_mask]\n",
    "\n",
    "    kcatkm_rmse = torch.sqrt(torch.mean((valid_labels - valid_preds) ** 2))\n",
    "\n",
    "    return 'rmse_kcatkm', kcatkm_rmse.item(), False\n",
    "\n",
    "\n",
    "def cal_grad(preds, train_data, ep=0):\n",
    "    labels = torch.tensor(train_data.get_label(), device=device)\n",
    "    preds = torch.tensor(preds, device=device)\n",
    "    labels = labels.view(num_tasks, -1).T\n",
    "    preds = preds.view(num_tasks, -1).T\n",
    "\n",
    "    # mask\n",
    "    valid_mask = labels != fill_nan_value\n",
    "    grad = torch.zeros_like(preds)\n",
    "    grad[valid_mask] = preds[valid_mask] - labels[valid_mask]\n",
    "\n",
    "    # sum\n",
    "    grad_final = grad.mean(dim=1)\n",
    "\n",
    "    # Hessian\n",
    "    grad_flattened = grad.T.flatten()\n",
    "    hess = torch.ones_like(grad_final)\n",
    "    hess2 = torch.ones_like(grad_flattened)\n",
    "\n",
    "    return grad_final.cpu().numpy(), hess.cpu().numpy(), grad_flattened.cpu().numpy(), hess2.cpu().numpy()\n",
    "\n",
    "\n",
    "# TODO Train model\n",
    "def train_mtgbm(params):\n",
    "    temp_params = deepcopy(params)\n",
    "    temp_params.update({\"verbosity\": -1, \"objective\": \"custom\", \"num_labels\": num_tasks, \"tree_learner\": 'serial2', \"num_threads\": num_threads})\n",
    "    num_iterations = temp_params.pop(\"num_iterations\")\n",
    "\n",
    "    task_scores_dict = {task_name: {'val': {name: [] for name in score_names}, 'test': {name: [] for name in score_names}} for task_name in task_names}\n",
    "\n",
    "    for fold_idx, (train_index, val_index) in enumerate(kf.split(train_val_x), start=1):\n",
    "        print(f\"Fold {fold_idx}\")\n",
    "        # split dataset\n",
    "        train_x, val_x = train_val_x[train_index], train_val_x[val_index]\n",
    "        train_y, val_y = train_val_y[train_index], train_val_y[val_index]\n",
    "\n",
    "        train_data = lgb.Dataset(train_x, label=train_y)\n",
    "\n",
    "        val_data = lgb.Dataset(val_x, label=val_y)\n",
    "\n",
    "        # get the best epoch number\n",
    "        evals_result_mt = {}\n",
    "        lgb.train(temp_params, train_data, num_iterations, valid_sets=[val_data],\n",
    "                  fobj=cal_grad, feval=self_kcatkm_rmse, verbose_eval=1000, evals_result=evals_result_mt,\n",
    "                  callbacks=[lgb.early_stopping(stopping_rounds=500)])\n",
    "        valid_records = evals_result_mt['valid_0']['rmse_kcatkm']\n",
    "        min_index = np.argmin(np.array(valid_records))\n",
    "        print(f\"valid_records min_index {min_index}\")\n",
    "\n",
    "        # train model for all scores of validation and test\n",
    "        train_data = lgb.Dataset(train_x, label=train_y)\n",
    "        val_data = lgb.Dataset(val_x, label=val_y)\n",
    "        evals_result_mt = {}\n",
    "        model = lgb.train(temp_params, train_data, min_index + 1, valid_sets=[val_data],\n",
    "                          fobj=cal_grad, feval=self_kcatkm_rmse, verbose_eval=1000,\n",
    "                          evals_result=evals_result_mt)\n",
    "        model.set_num_labels(num_tasks)\n",
    "\n",
    "        # validation predict\n",
    "        val_predicted = model.predict(val_x)\n",
    "        val_scores = {task_name: return_scores(val_y[:, idx], val_predicted[:, idx]) for idx, task_name in\n",
    "                      enumerate(task_names)}\n",
    "\n",
    "        # test predict\n",
    "        test_predicted = model.predict(test_x)\n",
    "        test_scores = {task_name: return_scores(test_y[:, idx], test_predicted[:, idx]) for idx, task_name in\n",
    "                       enumerate(task_names)}\n",
    "\n",
    "        # record\n",
    "        for task_name in task_names:\n",
    "            for score_idx, score_name in enumerate(score_names):\n",
    "                task_scores_dict[task_name]['val'][score_name].append(val_scores[task_name][score_idx])\n",
    "                task_scores_dict[task_name]['test'][score_name].append(test_scores[task_name][score_idx])\n",
    "        print(f\"Val  {val_scores} \\n Test {test_scores}\\n\")\n",
    "\n",
    "    print_scores(task_scores_dict)\n",
    "\n",
    "\n",
    "# init seed\n",
    "random_state = 66\n",
    "random.seed(random_state)\n",
    "np.random.seed(random_state)\n",
    "torch.manual_seed(random_state)\n",
    "torch.cuda.manual_seed(random_state)\n",
    "torch.cuda.manual_seed_all(random_state)\n",
    "\n",
    "# config\n",
    "protein_column,  substrate_column = 'prott5', 'molebert'\n",
    "input_model = 'mtgbm_km_kcat_kcatkm'\n",
    "dataset_path = f'{current_dir}/../../data_process/dataset/df_all_log_transformed.pkl'\n",
    "params_json_path = f'{current_dir}/../{input_model}_params.json'\n",
    "use_t_ph_embedding = True\n",
    "use_mw_logp = True\n",
    "num_threads = 32\n",
    "search_max_evals = 60\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Current device is {device}\")\n",
    "\n",
    "# input\n",
    "score_names = ['rmse', 'mae', 'r2', 'pcc']\n",
    "task_names = ['logkm', 'logkcatkm']\n",
    "num_tasks = len(task_names)\n",
    "df_input = pd.read_pickle(dataset_path)\n",
    "fill_nan_value = -100\n",
    "df_input = df_input.fillna(fill_nan_value)\n",
    "\n",
    "# split dataset\n",
    "df_train_val, df_test = train_test_split(df_input, test_size=0.2, random_state=random_state)\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=random_state)\n",
    "train_val_x, train_val_y = return_mtgbm_x_y(df_train_val, task_names)\n",
    "test_x, test_y = return_mtgbm_x_y(df_test, task_names)\n",
    "\n",
    "if os.path.exists(params_json_path):\n",
    "    with open(params_json_path, 'r') as json_file:\n",
    "        best_params = json.load(json_file)\n",
    "\n",
    "    print('best_params:', best_params)\n",
    "    print('using km kcatkm resample')\n",
    "    train_mtgbm(best_params)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current device is cuda:0\n",
      "best_params: {'bagging_fraction': 0.729611058732434, 'feature_fraction': 0.6643005188332146, 'lambda_l1': 0.346846951564011, 'lambda_l2': 0.7149783548509333, 'learning_rate': 0.07838547411322133, 'max_bin': 95, 'max_depth': 9, 'min_data_in_leaf': 21, 'num_iterations': 3273, 'num_leaves': 2350}\n",
      "using km kcatkm resample\n",
      "Fold 1\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[1000]\tvalid_0's rmse_kcatkm: 2.84422\n",
      "[2000]\tvalid_0's rmse_kcatkm: 2.79402\n",
      "[3000]\tvalid_0's rmse_kcatkm: 2.79064\n",
      "Early stopping, best iteration is:\n",
      "[2593]\tvalid_0's rmse_kcatkm: 2.78954\n",
      "valid_records min_index 2592\n",
      "[1000]\tvalid_0's rmse_kcatkm: 2.84422\n",
      "[2000]\tvalid_0's rmse_kcatkm: 2.79402\n",
      "inner_predict 14892\n",
      "inner_predict 18614\n",
      "Val  {'logkm': (1.7477696777051617, 1.224480794635082, 0.6522648013057186, 0.8097108318659493), 'logkcatkm': (2.789541664690702, 1.9826019033621638, 0.56319597773944, 0.751156411194851)} \n",
      " Test {'logkm': (1.7513974584370253, 1.2297536640597408, 0.6585994141080287, 0.8134328042818049), 'logkcatkm': (2.803276683240372, 2.007610198910901, 0.5385438917312704, 0.7356546333211222)}\n",
      "\n",
      "Fold 2\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[1000]\tvalid_0's rmse_kcatkm: 2.7665\n",
      "[2000]\tvalid_0's rmse_kcatkm: 2.7201\n",
      "[3000]\tvalid_0's rmse_kcatkm: 2.71344\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[3232]\tvalid_0's rmse_kcatkm: 2.71298\n",
      "valid_records min_index 3231\n",
      "[1000]\tvalid_0's rmse_kcatkm: 2.7665\n",
      "[2000]\tvalid_0's rmse_kcatkm: 2.7201\n",
      "[3000]\tvalid_0's rmse_kcatkm: 2.71344\n",
      "inner_predict 14892\n",
      "inner_predict 18614\n",
      "Val  {'logkm': (1.7590682280284802, 1.2262989311824546, 0.6636829903932842, 0.8158732533952986), 'logkcatkm': (2.712979151797301, 1.942683858685754, 0.5749417532368921, 0.7614355738299663)} \n",
      " Test {'logkm': (1.7292493403987745, 1.2144523285194906, 0.6671794996240519, 0.8185491047451684), 'logkcatkm': (2.790363500173432, 1.9909693282760998, 0.5427854584635425, 0.7392512936884896)}\n",
      "\n",
      "Fold 3\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[1000]\tvalid_0's rmse_kcatkm: 2.69489\n",
      "[2000]\tvalid_0's rmse_kcatkm: 2.63945\n",
      "[3000]\tvalid_0's rmse_kcatkm: 2.62674\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[3269]\tvalid_0's rmse_kcatkm: 2.62494\n",
      "valid_records min_index 3268\n",
      "[1000]\tvalid_0's rmse_kcatkm: 2.69489\n",
      "[2000]\tvalid_0's rmse_kcatkm: 2.63945\n",
      "[3000]\tvalid_0's rmse_kcatkm: 2.62674\n",
      "inner_predict 14890\n",
      "inner_predict 18614\n",
      "Val  {'logkm': (1.7261257617313035, 1.2195249179669285, 0.659824209403944, 0.8133611724302767), 'logkcatkm': (2.6249413363742176, 1.884582991987936, 0.5920954034845708, 0.7703307287646306)} \n",
      " Test {'logkm': (1.745675704857247, 1.2294574421705808, 0.6608264576090059, 0.8148466982753941), 'logkcatkm': (2.7758085508821777, 1.9805181928612547, 0.5475428160096654, 0.7422501956982768)}\n",
      "\n",
      "Fold 4\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[1000]\tvalid_0's rmse_kcatkm: 2.79187\n",
      "[2000]\tvalid_0's rmse_kcatkm: 2.74684\n",
      "[3000]\tvalid_0's rmse_kcatkm: 2.73482\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[3273]\tvalid_0's rmse_kcatkm: 2.73381\n",
      "valid_records min_index 3272\n",
      "[1000]\tvalid_0's rmse_kcatkm: 2.79187\n",
      "[2000]\tvalid_0's rmse_kcatkm: 2.74684\n",
      "[3000]\tvalid_0's rmse_kcatkm: 2.73482\n",
      "inner_predict 14890\n",
      "inner_predict 18614\n",
      "Val  {'logkm': (1.733703331320763, 1.224708131215923, 0.6619763272372317, 0.8148971375877849), 'logkcatkm': (2.7338119509252707, 1.9758746917163634, 0.5578709207414699, 0.7486459721883153)} \n",
      " Test {'logkm': (1.737575147351075, 1.2248214845638348, 0.6639669264824842, 0.816590409690188), 'logkcatkm': (2.77900576487199, 1.9782352268412056, 0.5464999230866985, 0.7417962770004893)}\n",
      "\n",
      "Fold 5\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[1000]\tvalid_0's rmse_kcatkm: 2.77236\n",
      "[2000]\tvalid_0's rmse_kcatkm: 2.72573\n",
      "[3000]\tvalid_0's rmse_kcatkm: 2.72255\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[3273]\tvalid_0's rmse_kcatkm: 2.7222\n",
      "valid_records min_index 3272\n",
      "[1000]\tvalid_0's rmse_kcatkm: 2.77236\n",
      "[2000]\tvalid_0's rmse_kcatkm: 2.72573\n",
      "[3000]\tvalid_0's rmse_kcatkm: 2.72255\n",
      "inner_predict 14890\n",
      "inner_predict 18614\n",
      "Val  {'logkm': (1.7432296087978498, 1.227043365675086, 0.6532009931691707, 0.811236118176347), 'logkcatkm': (2.7221984194586533, 1.9204440425725844, 0.5743052977831602, 0.759824398907367)} \n",
      " Test {'logkm': (1.7237128317090398, 1.2139090056297661, 0.6693072596143228, 0.8198588465687542), 'logkcatkm': (2.7588210953434005, 1.9726514571458644, 0.5530637857619889, 0.7452406540782646)}\n",
      "\n",
      "logkm\t RMSE\t MAE\t R2\t PCC\t\n",
      "Val  1.7420\t 1.2244\t 0.6582\t 0.8130\t\n",
      "Test 1.7375\t 1.2225\t 0.6640\t 0.8167\t\n",
      "\n",
      "logkcatkm\t RMSE\t MAE\t R2\t PCC\t\n",
      "Val  2.7167\t 1.9412\t 0.5725\t 0.7583\t\n",
      "Test 2.7815\t 1.9860\t 0.5457\t 0.7408\t\n",
      "\n"
     ]
    }
   ],
   "execution_count": 1
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DynoMTGBM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "db0e6cebfd42dbe7de32cf1b0daf517db5c30eda4a99fad3eb7c5d8b4a7bde0f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
