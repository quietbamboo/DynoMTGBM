{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb1f96913c6945c4",
   "metadata": {},
   "source": [
    "# MTGBM"
   ]
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-28T10:04:40.471783Z",
     "start_time": "2025-03-28T09:21:55.069842Z"
    }
   },
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import random\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "from scipy.stats import pearsonr\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "import lightgbmmt as lgb\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "\n",
    "def return_mtgbm_x_y(df_data, tasks):\n",
    "    y = np.array(df_data[tasks].values)\n",
    "\n",
    "    auxiliary_data = []\n",
    "    if use_t_ph_embedding:\n",
    "        ph = df_data['ph'].values.reshape(-1, 1)\n",
    "        t = df_data['t'].values.reshape(-1, 1)\n",
    "        auxiliary_data.append(ph)\n",
    "        auxiliary_data.append(t)\n",
    "\n",
    "    if use_mw_logp:\n",
    "        mw = df_data['mw'].values.reshape(-1, 1)\n",
    "        logp = df_data['logp'].values.reshape(-1, 1)\n",
    "        auxiliary_data.append(mw)\n",
    "        auxiliary_data.append(logp)\n",
    "\n",
    "    protein_data = np.array(df_data[protein_column].tolist())\n",
    "    substrate_data = np.array(df_data[substrate_column].tolist())\n",
    "\n",
    "    x = np.hstack([protein_data, substrate_data] + auxiliary_data)\n",
    "    return x, y\n",
    "\n",
    "def return_scores(y_true, y_pred):\n",
    "    mask = y_true != fill_nan_value\n",
    "    y_true = y_true[mask]\n",
    "    y_pred = y_pred[mask]\n",
    "\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    pcc = pearsonr(y_true, y_pred)[0]\n",
    "\n",
    "    return rmse, mae, r2, pcc\n",
    "\n",
    "def print_scores(task_scores_dict):\n",
    "    for task_name in task_names:\n",
    "        print(f\"{task_name}\\t RMSE\\t MAE\\t R2\\t PCC\\t\")\n",
    "\n",
    "        task_val_scores = task_scores_dict[task_name]['val']\n",
    "        task_test_scores = task_scores_dict[task_name]['test']\n",
    "\n",
    "        val_metrics = [f\"{np.mean(task_val_scores[metric_name]):.4f}\\t\" for metric_name in\n",
    "                       score_names]\n",
    "        print(\"Val  \" + \" \".join(val_metrics))\n",
    "\n",
    "        test_metrics = [f\"{np.mean(task_test_scores[metric_name]):.4f}\\t\" for metric_name in\n",
    "                        score_names]\n",
    "        print(\"Test \" + \" \".join(test_metrics))\n",
    "        print()\n",
    "\n",
    "def self_kcatkm_rmse(preds, train_data):\n",
    "    labels = torch.tensor(train_data.get_label(), device=device)\n",
    "    preds = torch.tensor(preds, device=device)\n",
    "\n",
    "    # extract kcatkm values\n",
    "    labels = labels.view(num_tasks, -1).T[:, num_tasks - 1]\n",
    "    preds = preds.view(num_tasks, -1).T[:, num_tasks - 1]\n",
    "\n",
    "    # mask\n",
    "    valid_mask = labels != fill_nan_value\n",
    "    valid_labels = labels[valid_mask]\n",
    "    valid_preds = preds[valid_mask]\n",
    "\n",
    "    kcatkm_rmse = torch.sqrt(torch.mean((valid_labels - valid_preds) ** 2))\n",
    "\n",
    "    return 'rmse_kcatkm', kcatkm_rmse.item(), False\n",
    "\n",
    "\n",
    "def cal_grad(preds, train_data, ep=0):\n",
    "    labels = torch.tensor(train_data.get_label(), device=device)\n",
    "    preds = torch.tensor(preds, device=device)\n",
    "    labels = labels.view(num_tasks, -1).T\n",
    "    preds = preds.view(num_tasks, -1).T\n",
    "\n",
    "    # mask\n",
    "    valid_mask = labels != fill_nan_value\n",
    "    grad = torch.zeros_like(preds)\n",
    "    grad[valid_mask] = preds[valid_mask] - labels[valid_mask]\n",
    "\n",
    "    # sum\n",
    "    grad_final = grad.mean(dim=1)\n",
    "\n",
    "    # Hessian\n",
    "    grad_flattened = grad.T.flatten()\n",
    "    hess = torch.ones_like(grad_final)\n",
    "    hess2 = torch.ones_like(grad_flattened)\n",
    "\n",
    "    return grad_final.cpu().numpy(), hess.cpu().numpy(), grad_flattened.cpu().numpy(), hess2.cpu().numpy()\n",
    "\n",
    "\n",
    "# TODO Train model\n",
    "def train_mtgbm(params):\n",
    "    temp_params = deepcopy(params)\n",
    "    temp_params.update({\"verbosity\": -1, \"objective\": \"custom\", \"num_labels\": num_tasks, \"tree_learner\": 'serial2', \"num_threads\": num_threads})\n",
    "    num_iterations = temp_params.pop(\"num_iterations\")\n",
    "\n",
    "    task_scores_dict = {task_name: {'val': {name: [] for name in score_names}, 'test': {name: [] for name in score_names}} for task_name in task_names}\n",
    "\n",
    "    for fold_idx, (train_index, val_index) in enumerate(kf.split(train_val_x), start=1):\n",
    "        print(f\"Fold {fold_idx}\")\n",
    "        # split dataset\n",
    "        train_x, val_x = train_val_x[train_index], train_val_x[val_index]\n",
    "        train_y, val_y = train_val_y[train_index], train_val_y[val_index]\n",
    "\n",
    "        train_data = lgb.Dataset(train_x, label=train_y)\n",
    "\n",
    "        val_data = lgb.Dataset(val_x, label=val_y)\n",
    "\n",
    "        # get the best epoch number\n",
    "        evals_result_mt = {}\n",
    "        lgb.train(temp_params, train_data, num_iterations, valid_sets=[val_data],\n",
    "                  fobj=cal_grad, feval=self_kcatkm_rmse, verbose_eval=1000, evals_result=evals_result_mt,\n",
    "                  callbacks=[lgb.early_stopping(stopping_rounds=500)])\n",
    "        valid_records = evals_result_mt['valid_0']['rmse_kcatkm']\n",
    "        min_index = np.argmin(np.array(valid_records))\n",
    "        print(f\"valid_records min_index {min_index}\")\n",
    "\n",
    "        # train model for all scores of validation and test\n",
    "        train_data = lgb.Dataset(train_x, label=train_y)\n",
    "        val_data = lgb.Dataset(val_x, label=val_y)\n",
    "        evals_result_mt = {}\n",
    "        model = lgb.train(temp_params, train_data, min_index + 1, valid_sets=[val_data],\n",
    "                          fobj=cal_grad, feval=self_kcatkm_rmse, verbose_eval=1000,\n",
    "                          evals_result=evals_result_mt)\n",
    "        model.set_num_labels(num_tasks)\n",
    "\n",
    "        # validation predict\n",
    "        val_predicted = model.predict(val_x)\n",
    "        val_scores = {task_name: return_scores(val_y[:, idx], val_predicted[:, idx]) for idx, task_name in\n",
    "                      enumerate(task_names)}\n",
    "\n",
    "        # test predict\n",
    "        test_predicted = model.predict(test_x)\n",
    "        test_scores = {task_name: return_scores(test_y[:, idx], test_predicted[:, idx]) for idx, task_name in\n",
    "                       enumerate(task_names)}\n",
    "\n",
    "        # record\n",
    "        for task_name in task_names:\n",
    "            for score_idx, score_name in enumerate(score_names):\n",
    "                task_scores_dict[task_name]['val'][score_name].append(val_scores[task_name][score_idx])\n",
    "                task_scores_dict[task_name]['test'][score_name].append(test_scores[task_name][score_idx])\n",
    "        print(f\"Val  {val_scores} \\n Test {test_scores}\\n\")\n",
    "\n",
    "    print_scores(task_scores_dict)\n",
    "\n",
    "\n",
    "# init seed\n",
    "random_state = 66\n",
    "random.seed(random_state)\n",
    "np.random.seed(random_state)\n",
    "torch.manual_seed(random_state)\n",
    "torch.cuda.manual_seed(random_state)\n",
    "torch.cuda.manual_seed_all(random_state)\n",
    "\n",
    "# config\n",
    "protein_column,  substrate_column = 'prott5', 'molebert'\n",
    "input_model = 'mtgbm_km_kcat_kcatkm'\n",
    "dataset_path = f'{current_dir}/../../data_process/dataset/df_all_log_transformed.pkl'\n",
    "params_json_path = f'{current_dir}/../{input_model}_params.json'\n",
    "use_t_ph_embedding = True\n",
    "use_mw_logp = True\n",
    "num_threads = 32\n",
    "search_max_evals = 60\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Current device is {device}\")\n",
    "\n",
    "# input\n",
    "score_names = ['rmse', 'mae', 'r2', 'pcc']\n",
    "task_names = ['logkm', 'logkcat', 'logkcatkm']\n",
    "num_tasks = len(task_names)\n",
    "df_input = pd.read_pickle(dataset_path)\n",
    "df_input['logkm'] = -df_input['logkm']\n",
    "fill_nan_value = -100\n",
    "df_input = df_input.fillna(fill_nan_value)\n",
    "\n",
    "# split dataset\n",
    "df_train_val, df_test = train_test_split(df_input, test_size=0.2, random_state=random_state)\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=random_state)\n",
    "train_val_x, train_val_y = return_mtgbm_x_y(df_train_val, task_names)\n",
    "test_x, test_y = return_mtgbm_x_y(df_test, task_names)\n",
    "\n",
    "if os.path.exists(params_json_path):\n",
    "    with open(params_json_path, 'r') as json_file:\n",
    "        best_params = json.load(json_file)\n",
    "\n",
    "    print('best_params:', best_params)\n",
    "    print('using -km kcat kcatkm resample')\n",
    "    train_mtgbm(best_params)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current device is cuda:0\n",
      "best_params: {'bagging_fraction': 0.729611058732434, 'feature_fraction': 0.6643005188332146, 'lambda_l1': 0.346846951564011, 'lambda_l2': 0.7149783548509333, 'learning_rate': 0.07838547411322133, 'max_bin': 95, 'max_depth': 9, 'min_data_in_leaf': 21, 'num_iterations': 3273, 'num_leaves': 2350}\n",
      "using -km kcat kcatkm resample\n",
      "Fold 1\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[1000]\tvalid_0's rmse_kcatkm: 2.681\n",
      "[2000]\tvalid_0's rmse_kcatkm: 2.66536\n",
      "Early stopping, best iteration is:\n",
      "[2011]\tvalid_0's rmse_kcatkm: 2.66506\n",
      "valid_records min_index 2010\n",
      "[1000]\tvalid_0's rmse_kcatkm: 2.681\n",
      "[2000]\tvalid_0's rmse_kcatkm: 2.66536\n",
      "inner_predict 22338\n",
      "inner_predict 27921\n",
      "Val  {'logkm': (1.751801400303909, 1.2363345265824102, 0.6506586527402988, 0.8085104578640728), 'logkcat': (2.1498353062197597, 1.4751936901448115, 0.6199608228130944, 0.7876183586110916), 'logkcatkm': (2.6650555104066562, 1.8946390688802637, 0.6013117442462484, 0.7756966336682282)} \n",
      " Test {'logkm': (1.7328931708241135, 1.2308380105577508, 0.6657753999767203, 0.817267379687779), 'logkcat': (2.2021673788314264, 1.5170764162351627, 0.6242173945817282, 0.7901719397797586), 'logkcatkm': (2.6465028592202926, 1.8890815538698174, 0.5887146835978647, 0.7676684188854598)}\n",
      "\n",
      "Fold 2\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[1000]\tvalid_0's rmse_kcatkm: 2.59577\n",
      "[2000]\tvalid_0's rmse_kcatkm: 2.58767\n",
      "Early stopping, best iteration is:\n",
      "[1579]\tvalid_0's rmse_kcatkm: 2.58734\n",
      "valid_records min_index 1578\n",
      "[1000]\tvalid_0's rmse_kcatkm: 2.59577\n",
      "inner_predict 22338\n",
      "inner_predict 27921\n",
      "Val  {'logkm': (1.7754560773989332, 1.2601969468279812, 0.6573874009324673, 0.811403479766264), 'logkcat': (2.1050927865320355, 1.4773473829481152, 0.6318668783219755, 0.7951595397759417), 'logkcatkm': (2.5873439679476404, 1.8603650309422648, 0.6133982004317564, 0.7837513448998273)} \n",
      " Test {'logkm': (1.7307875929615903, 1.2465770930697242, 0.666587115768916, 0.8171839852497308), 'logkcat': (2.216019859964053, 1.5354597876693452, 0.619474890565618, 0.7873422988009515), 'logkcatkm': (2.6762681968719173, 1.891738618826747, 0.5794111684356188, 0.7616220182454873)}\n",
      "\n",
      "Fold 3\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[1000]\tvalid_0's rmse_kcatkm: 2.58473\n",
      "[2000]\tvalid_0's rmse_kcatkm: 2.55531\n",
      "[3000]\tvalid_0's rmse_kcatkm: 2.55211\n",
      "Early stopping, best iteration is:\n",
      "[2751]\tvalid_0's rmse_kcatkm: 2.55132\n",
      "valid_records min_index 2750\n",
      "[1000]\tvalid_0's rmse_kcatkm: 2.58473\n",
      "[2000]\tvalid_0's rmse_kcatkm: 2.55531\n",
      "inner_predict 22335\n",
      "inner_predict 27921\n",
      "Val  {'logkm': (1.7161823490348984, 1.2179305992533587, 0.6637321120801107, 0.8157934913062366), 'logkcat': (2.1790576202224154, 1.4887399029977333, 0.6156616535134464, 0.7857271540132662), 'logkcatkm': (2.5513214182252186, 1.7939252922847495, 0.6146549849997747, 0.7844399292788724)} \n",
      " Test {'logkm': (1.7369595554870456, 1.2307084710623266, 0.6642049853464724, 0.8166170930797388), 'logkcat': (2.177712940150259, 1.4888066386402001, 0.6325169717100059, 0.795568536454662), 'logkcatkm': (2.6391558147649077, 1.8551096016748807, 0.590995079500982, 0.7697403395371212)}\n",
      "\n",
      "Fold 4\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[1000]\tvalid_0's rmse_kcatkm: 2.64395\n",
      "[2000]\tvalid_0's rmse_kcatkm: 2.63047\n",
      "Early stopping, best iteration is:\n",
      "[2374]\tvalid_0's rmse_kcatkm: 2.6297\n",
      "valid_records min_index 2373\n",
      "[1000]\tvalid_0's rmse_kcatkm: 2.64395\n",
      "[2000]\tvalid_0's rmse_kcatkm: 2.63047\n",
      "inner_predict 22335\n",
      "inner_predict 27921\n",
      "Val  {'logkm': (1.7285940937165074, 1.231674669847824, 0.6639657084925978, 0.8158778666502831), 'logkcat': (2.0368911199165494, 1.4250132461722422, 0.6491607932956235, 0.8060788703155518), 'logkcatkm': (2.6296994306299286, 1.8779003373004992, 0.5909051341957765, 0.7690909446222043)} \n",
      " Test {'logkm': (1.727706619387106, 1.2285732681887473, 0.6677730752692586, 0.818541898389875), 'logkcat': (2.1811422614720093, 1.5066526528148962, 0.6313586834752165, 0.7947415384645894), 'logkcatkm': (2.6733589567593232, 1.8871940347996907, 0.5803250744833147, 0.7630768961132288)}\n",
      "\n",
      "Fold 5\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[1000]\tvalid_0's rmse_kcatkm: 2.63561\n",
      "[2000]\tvalid_0's rmse_kcatkm: 2.62212\n",
      "Early stopping, best iteration is:\n",
      "[2091]\tvalid_0's rmse_kcatkm: 2.621\n",
      "valid_records min_index 2090\n",
      "[1000]\tvalid_0's rmse_kcatkm: 2.63561\n",
      "[2000]\tvalid_0's rmse_kcatkm: 2.62212\n",
      "inner_predict 22335\n",
      "inner_predict 27921\n",
      "Val  {'logkm': (1.7357058199394835, 1.232273799372687, 0.6561881059830411, 0.8120721258510716), 'logkcat': (2.123279149002957, 1.4727223881634062, 0.6453173702270883, 0.8040329007472311), 'logkcatkm': (2.621002886880988, 1.8551799084091158, 0.6053667364807872, 0.7795853716019168)} \n",
      " Test {'logkm': (1.7362799424895392, 1.2401525415115568, 0.6644677041993261, 0.8163128089017302), 'logkcat': (2.180567486052297, 1.5046042863369495, 0.6315529468692525, 0.7947424545681487), 'logkcatkm': (2.6304760391571445, 1.8659170144202377, 0.5936809634455735, 0.7707901773150169)}\n",
      "\n",
      "logkm\t RMSE\t MAE\t R2\t PCC\t\n",
      "Val  1.7415\t 1.2357\t 0.6584\t 0.8127\t\n",
      "Test 1.7329\t 1.2354\t 0.6658\t 0.8172\t\n",
      "\n",
      "logkcat\t RMSE\t MAE\t R2\t PCC\t\n",
      "Val  2.1188\t 1.4678\t 0.6324\t 0.7957\t\n",
      "Test 2.1915\t 1.5105\t 0.6278\t 0.7925\t\n",
      "\n",
      "logkcatkm\t RMSE\t MAE\t R2\t PCC\t\n",
      "Val  2.6109\t 1.8564\t 0.6051\t 0.7785\t\n",
      "Test 2.6532\t 1.8778\t 0.5866\t 0.7666\t\n",
      "\n"
     ]
    }
   ],
   "execution_count": 1
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DynoMTGBM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "db0e6cebfd42dbe7de32cf1b0daf517db5c30eda4a99fad3eb7c5d8b4a7bde0f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
