{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb1f96913c6945c4",
   "metadata": {},
   "source": [
    "# MTGBM"
   ]
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-28T14:20:34.427685Z",
     "start_time": "2025-03-28T13:39:29.336769Z"
    }
   },
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import random\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "from scipy.stats import pearsonr\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "import lightgbmmt as lgb\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "\n",
    "def return_mtgbm_x_y(df_data, tasks):\n",
    "    y = np.array(df_data[tasks].values)\n",
    "\n",
    "    auxiliary_data = []\n",
    "    if use_t_ph_embedding:\n",
    "        ph = df_data['ph'].values.reshape(-1, 1)\n",
    "        t = df_data['t'].values.reshape(-1, 1)\n",
    "        auxiliary_data.append(ph)\n",
    "        auxiliary_data.append(t)\n",
    "\n",
    "    if use_mw_logp:\n",
    "        mw = df_data['mw'].values.reshape(-1, 1)\n",
    "        logp = df_data['logp'].values.reshape(-1, 1)\n",
    "        auxiliary_data.append(mw)\n",
    "        auxiliary_data.append(logp)\n",
    "\n",
    "    protein_data = np.array(df_data[protein_column].tolist())\n",
    "    substrate_data = np.array(df_data[substrate_column].tolist())\n",
    "\n",
    "    x = np.hstack([protein_data, substrate_data] + auxiliary_data)\n",
    "    return x, y\n",
    "\n",
    "def return_scores(y_true, y_pred):\n",
    "    mask = y_true != fill_nan_value\n",
    "    y_true = y_true[mask]\n",
    "    y_pred = y_pred[mask]\n",
    "\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    pcc = pearsonr(y_true, y_pred)[0]\n",
    "\n",
    "    return rmse, mae, r2, pcc\n",
    "\n",
    "def print_scores(task_scores_dict):\n",
    "    for task_name in task_names:\n",
    "        print(f\"{task_name}\\t RMSE\\t MAE\\t R2\\t PCC\\t\")\n",
    "\n",
    "        task_val_scores = task_scores_dict[task_name]['val']\n",
    "        task_test_scores = task_scores_dict[task_name]['test']\n",
    "\n",
    "        val_metrics = [f\"{np.mean(task_val_scores[metric_name]):.4f}\\t\" for metric_name in\n",
    "                       score_names]\n",
    "        print(\"Val  \" + \" \".join(val_metrics))\n",
    "\n",
    "        test_metrics = [f\"{np.mean(task_test_scores[metric_name]):.4f}\\t\" for metric_name in\n",
    "                        score_names]\n",
    "        print(\"Test \" + \" \".join(test_metrics))\n",
    "        print()\n",
    "\n",
    "def self_kcatkm_rmse(preds, train_data):\n",
    "    labels = torch.tensor(train_data.get_label(), device=device)\n",
    "    preds = torch.tensor(preds, device=device)\n",
    "\n",
    "    # extract kcatkm values\n",
    "    labels = labels.view(num_tasks, -1).T[:, num_tasks - 1]\n",
    "    preds = preds.view(num_tasks, -1).T[:, num_tasks - 1]\n",
    "\n",
    "    # mask\n",
    "    valid_mask = labels != fill_nan_value\n",
    "    valid_labels = labels[valid_mask]\n",
    "    valid_preds = preds[valid_mask]\n",
    "\n",
    "    kcatkm_rmse = torch.sqrt(torch.mean((valid_labels - valid_preds) ** 2))\n",
    "\n",
    "    return 'rmse_kcatkm', kcatkm_rmse.item(), False\n",
    "\n",
    "\n",
    "def cal_grad(preds, train_data, ep=0):\n",
    "    labels = torch.tensor(train_data.get_label(), device=device)\n",
    "    preds = torch.tensor(preds, device=device)\n",
    "    labels = labels.view(num_tasks, -1).T\n",
    "    preds = preds.view(num_tasks, -1).T\n",
    "\n",
    "    # mask\n",
    "    valid_mask = labels != fill_nan_value\n",
    "    grad = torch.zeros_like(preds)\n",
    "    grad[valid_mask] = preds[valid_mask] - labels[valid_mask]\n",
    "\n",
    "    # sum\n",
    "    grad_final = grad.mean(dim=1)\n",
    "\n",
    "    # Hessian\n",
    "    grad_flattened = grad.T.flatten()\n",
    "    hess = torch.ones_like(grad_final)\n",
    "    hess2 = torch.ones_like(grad_flattened)\n",
    "\n",
    "    return grad_final.cpu().numpy(), hess.cpu().numpy(), grad_flattened.cpu().numpy(), hess2.cpu().numpy()\n",
    "\n",
    "\n",
    "# TODO Train model\n",
    "def train_mtgbm(params):\n",
    "    temp_params = deepcopy(params)\n",
    "    temp_params.update({\"verbosity\": -1, \"objective\": \"custom\", \"num_labels\": num_tasks, \"tree_learner\": 'serial2', \"num_threads\": num_threads})\n",
    "    num_iterations = temp_params.pop(\"num_iterations\")\n",
    "\n",
    "    task_scores_dict = {task_name: {'val': {name: [] for name in score_names}, 'test': {name: [] for name in score_names}} for task_name in task_names}\n",
    "\n",
    "    for fold_idx, (train_index, val_index) in enumerate(kf.split(train_val_x), start=1):\n",
    "        print(f\"Fold {fold_idx}\")\n",
    "        # split dataset\n",
    "        train_x, val_x = train_val_x[train_index], train_val_x[val_index]\n",
    "        train_y, val_y = train_val_y[train_index], train_val_y[val_index]\n",
    "\n",
    "        train_data = lgb.Dataset(train_x, label=train_y)\n",
    "\n",
    "        val_data = lgb.Dataset(val_x, label=val_y)\n",
    "\n",
    "        # get the best epoch number\n",
    "        evals_result_mt = {}\n",
    "        lgb.train(temp_params, train_data, num_iterations, valid_sets=[val_data],\n",
    "                  fobj=cal_grad, feval=self_kcatkm_rmse, verbose_eval=1000, evals_result=evals_result_mt,\n",
    "                  callbacks=[lgb.early_stopping(stopping_rounds=500)])\n",
    "        valid_records = evals_result_mt['valid_0']['rmse_kcatkm']\n",
    "        min_index = np.argmin(np.array(valid_records))\n",
    "        print(f\"valid_records min_index {min_index}\")\n",
    "\n",
    "        # train model for all scores of validation and test\n",
    "        train_data = lgb.Dataset(train_x, label=train_y)\n",
    "        val_data = lgb.Dataset(val_x, label=val_y)\n",
    "        evals_result_mt = {}\n",
    "        model = lgb.train(temp_params, train_data, min_index + 1, valid_sets=[val_data],\n",
    "                          fobj=cal_grad, feval=self_kcatkm_rmse, verbose_eval=1000,\n",
    "                          evals_result=evals_result_mt)\n",
    "        model.set_num_labels(num_tasks)\n",
    "\n",
    "        # validation predict\n",
    "        val_predicted = model.predict(val_x)\n",
    "        val_scores = {task_name: return_scores(val_y[:, idx], val_predicted[:, idx]) for idx, task_name in\n",
    "                      enumerate(task_names)}\n",
    "\n",
    "        # test predict\n",
    "        test_predicted = model.predict(test_x)\n",
    "        test_scores = {task_name: return_scores(test_y[:, idx], test_predicted[:, idx]) for idx, task_name in\n",
    "                       enumerate(task_names)}\n",
    "\n",
    "        # record\n",
    "        for task_name in task_names:\n",
    "            for score_idx, score_name in enumerate(score_names):\n",
    "                task_scores_dict[task_name]['val'][score_name].append(val_scores[task_name][score_idx])\n",
    "                task_scores_dict[task_name]['test'][score_name].append(test_scores[task_name][score_idx])\n",
    "        print(f\"Val  {val_scores} \\n Test {test_scores}\\n\")\n",
    "\n",
    "    print_scores(task_scores_dict)\n",
    "\n",
    "\n",
    "# init seed\n",
    "random_state = 66\n",
    "random.seed(random_state)\n",
    "np.random.seed(random_state)\n",
    "torch.manual_seed(random_state)\n",
    "torch.cuda.manual_seed(random_state)\n",
    "torch.cuda.manual_seed_all(random_state)\n",
    "\n",
    "# config\n",
    "protein_column,  substrate_column = 'prott5', 'molebert'\n",
    "input_model = 'mtgbm_km_kcat_kcatkm'\n",
    "dataset_path = f'{current_dir}/../../data_process/dataset/df_all_log_transformed.pkl'\n",
    "params_json_path = f'{current_dir}/../{input_model}_params.json'\n",
    "use_t_ph_embedding = True\n",
    "use_mw_logp = True\n",
    "num_threads = 32\n",
    "search_max_evals = 60\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Current device is {device}\")\n",
    "\n",
    "# input\n",
    "score_names = ['rmse', 'mae', 'r2', 'pcc']\n",
    "task_names = ['logkm', 'logkcatkm']\n",
    "num_tasks = len(task_names)\n",
    "df_input = pd.read_pickle(dataset_path)\n",
    "df_input['logkm'] = -df_input['logkm']\n",
    "fill_nan_value = -100\n",
    "df_input = df_input.fillna(fill_nan_value)\n",
    "\n",
    "# split dataset\n",
    "df_train_val, df_test = train_test_split(df_input, test_size=0.2, random_state=random_state)\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=random_state)\n",
    "train_val_x, train_val_y = return_mtgbm_x_y(df_train_val, task_names)\n",
    "test_x, test_y = return_mtgbm_x_y(df_test, task_names)\n",
    "\n",
    "if os.path.exists(params_json_path):\n",
    "    with open(params_json_path, 'r') as json_file:\n",
    "        best_params = json.load(json_file)\n",
    "\n",
    "    print('best_params:', best_params)\n",
    "    print('using -km kcatkm resample')\n",
    "    train_mtgbm(best_params)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current device is cuda:0\n",
      "best_params: {'bagging_fraction': 0.729611058732434, 'feature_fraction': 0.6643005188332146, 'lambda_l1': 0.346846951564011, 'lambda_l2': 0.7149783548509333, 'learning_rate': 0.07838547411322133, 'max_bin': 95, 'max_depth': 9, 'min_data_in_leaf': 21, 'num_iterations': 3273, 'num_leaves': 2350}\n",
      "using -km kcatkm resample\n",
      "Fold 1\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[1000]\tvalid_0's rmse_kcatkm: 2.73093\n",
      "[2000]\tvalid_0's rmse_kcatkm: 2.70831\n",
      "[3000]\tvalid_0's rmse_kcatkm: 2.7035\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[3267]\tvalid_0's rmse_kcatkm: 2.70293\n",
      "valid_records min_index 3266\n",
      "[1000]\tvalid_0's rmse_kcatkm: 2.73093\n",
      "[2000]\tvalid_0's rmse_kcatkm: 2.70831\n",
      "[3000]\tvalid_0's rmse_kcatkm: 2.7035\n",
      "inner_predict 14892\n",
      "inner_predict 18614\n",
      "Val  {'logkm': (1.7531906738758773, 1.2184128797636933, 0.650104339736495, 0.8099216780569887), 'logkcatkm': (2.7029325050365203, 1.9028042042589104, 0.5898985298918944, 0.7686763523650543)} \n",
      " Test {'logkm': (1.7434542830260538, 1.2167864131880062, 0.6616891242748415, 0.8165506939353906), 'logkcatkm': (2.6832967832907255, 1.9106407855277714, 0.5771991130360861, 0.7609505432260975)}\n",
      "\n",
      "Fold 2\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[1000]\tvalid_0's rmse_kcatkm: 2.61732\n",
      "[2000]\tvalid_0's rmse_kcatkm: 2.60304\n",
      "Early stopping, best iteration is:\n",
      "[2244]\tvalid_0's rmse_kcatkm: 2.60213\n",
      "valid_records min_index 2243\n",
      "[1000]\tvalid_0's rmse_kcatkm: 2.61732\n",
      "[2000]\tvalid_0's rmse_kcatkm: 2.60304\n",
      "inner_predict 14892\n",
      "inner_predict 18614\n",
      "Val  {'logkm': (1.7609760775549346, 1.2220997164503002, 0.6629530696394037, 0.8162740240578894), 'logkcatkm': (2.6021330909802804, 1.8648494818015353, 0.6089659779520714, 0.7815267942254566)} \n",
      " Test {'logkm': (1.7315849158325065, 1.217887420119598, 0.6662798580199691, 0.8186031823268831), 'logkcatkm': (2.7214745134705716, 1.924602229228661, 0.5650823700655108, 0.7530674165046531)}\n",
      "\n",
      "Fold 3\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[1000]\tvalid_0's rmse_kcatkm: 2.61274\n",
      "[2000]\tvalid_0's rmse_kcatkm: 2.58842\n",
      "[3000]\tvalid_0's rmse_kcatkm: 2.58643\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[3042]\tvalid_0's rmse_kcatkm: 2.58616\n",
      "valid_records min_index 3041\n",
      "[1000]\tvalid_0's rmse_kcatkm: 2.61274\n",
      "[2000]\tvalid_0's rmse_kcatkm: 2.58842\n",
      "[3000]\tvalid_0's rmse_kcatkm: 2.58643\n",
      "inner_predict 14890\n",
      "inner_predict 18614\n",
      "Val  {'logkm': (1.7093993504729224, 1.2061912205186343, 0.6663849736536587, 0.8180829640644118), 'logkcatkm': (2.586158481483946, 1.8176681267868366, 0.6040597384263754, 0.7779156786685624)} \n",
      " Test {'logkm': (1.7387147367825186, 1.2187146800474777, 0.6635260070605197, 0.8169943573737688), 'logkcatkm': (2.6887287805186175, 1.8896811416591557, 0.5754855658032678, 0.7600280053399167)}\n",
      "\n",
      "Fold 4\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[1000]\tvalid_0's rmse_kcatkm: 2.68227\n",
      "[2000]\tvalid_0's rmse_kcatkm: 2.65572\n",
      "[3000]\tvalid_0's rmse_kcatkm: 2.65268\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[3127]\tvalid_0's rmse_kcatkm: 2.65209\n",
      "valid_records min_index 3126\n",
      "[1000]\tvalid_0's rmse_kcatkm: 2.68227\n",
      "[2000]\tvalid_0's rmse_kcatkm: 2.65572\n",
      "[3000]\tvalid_0's rmse_kcatkm: 2.65268\n",
      "inner_predict 14890\n",
      "inner_predict 18614\n",
      "Val  {'logkm': (1.729583506242649, 1.214117958431003, 0.6635809198485962, 0.8166070139011193), 'logkcatkm': (2.6520886367985796, 1.8953625596418577, 0.583909429462268, 0.764849807758375)} \n",
      " Test {'logkm': (1.7231215465964025, 1.2135754366074074, 0.6695340957249498, 0.8205495041197822), 'logkcatkm': (2.7064561835945007, 1.9157441370006936, 0.5698692705945816, 0.7569615317060845)}\n",
      "\n",
      "Fold 5\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[1000]\tvalid_0's rmse_kcatkm: 2.67976\n",
      "[2000]\tvalid_0's rmse_kcatkm: 2.64876\n",
      "[3000]\tvalid_0's rmse_kcatkm: 2.64616\n",
      "Early stopping, best iteration is:\n",
      "[2727]\tvalid_0's rmse_kcatkm: 2.64555\n",
      "valid_records min_index 2726\n",
      "[1000]\tvalid_0's rmse_kcatkm: 2.67976\n",
      "[2000]\tvalid_0's rmse_kcatkm: 2.64876\n",
      "inner_predict 14890\n",
      "inner_predict 18614\n",
      "Val  {'logkm': (1.7386773379138825, 1.2188386677688512, 0.6550098904874447, 0.8130522746693856), 'logkcatkm': (2.645553366007779, 1.868962506225575, 0.5979391897998623, 0.7756512791614711)} \n",
      " Test {'logkm': (1.7247693899749441, 1.2143797271928811, 0.6689017358821276, 0.8202999856888278), 'logkcatkm': (2.6890032143297895, 1.9106414306010595, 0.5753989024994417, 0.7596765983417084)}\n",
      "\n",
      "logkm\t RMSE\t MAE\t R2\t PCC\t\n",
      "Val  1.7384\t 1.2159\t 0.6596\t 0.8148\t\n",
      "Test 1.7323\t 1.2163\t 0.6660\t 0.8186\t\n",
      "\n",
      "logkcatkm\t RMSE\t MAE\t R2\t PCC\t\n",
      "Val  2.6378\t 1.8699\t 0.5970\t 0.7737\t\n",
      "Test 2.6978\t 1.9103\t 0.5726\t 0.7581\t\n",
      "\n"
     ]
    }
   ],
   "execution_count": 1
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DynoMTGBM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "db0e6cebfd42dbe7de32cf1b0daf517db5c30eda4a99fad3eb7c5d8b4a7bde0f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
