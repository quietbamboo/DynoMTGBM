{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/coder/miniconda/envs/km_predict/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current device is cuda:0\n",
      "best_params: {'bagging_fraction': 0.729611058732434, 'feature_fraction': 0.6643005188332146, 'lambda_l1': 0.346846951564011, 'lambda_l2': 0.7149783548509333, 'learning_rate': 0.07838547411322133, 'max_bin': 95, 'max_depth': 9, 'min_data_in_leaf': 21, 'num_iterations': 3273, 'num_leaves': 2350}\n",
      "using km kcat kcatkm resample\n",
      "Fold 1\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[1000]\tvalid_0's rmse_kcatkm: 2.80727\n",
      "[2000]\tvalid_0's rmse_kcatkm: 2.75144\n",
      "[3000]\tvalid_0's rmse_kcatkm: 2.73276\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[3213]\tvalid_0's rmse_kcatkm: 2.73165\n",
      "valid_records min_index 3212\n",
      "[1000]\tvalid_0's rmse_kcatkm: 2.80727\n",
      "[2000]\tvalid_0's rmse_kcatkm: 2.75144\n",
      "[3000]\tvalid_0's rmse_kcatkm: 2.73276\n",
      "inner_predict 22338\n",
      "inner_predict 27921\n",
      "Val  {'logkm': (1.9109556035647068, 1.3391179579914092, 0.5842986381589887, 0.7737220538127693), 'logkcat': (2.1186015596039125, 1.4489744861878824, 0.6309233562807753, 0.7946003718936082), 'logkcatkm': (2.7316490430045843, 1.9527615198222774, 0.5811382271569572, 0.7626667815067704)} \n",
      " Test {'logkm': (1.9434912758008545, 1.3484594089471984, 0.5796025987406258, 0.7717931190171108), 'logkcat': (2.211003647396647, 1.508249625853079, 0.6211956643580228, 0.7884435245769988), 'logkcatkm': (2.725400083191815, 1.96842826417986, 0.5638267780610522, 0.7513100712116378)}\n",
      "\n",
      "Fold 2\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[1000]\tvalid_0's rmse_kcatkm: 2.69186\n",
      "[2000]\tvalid_0's rmse_kcatkm: 2.64069\n",
      "[3000]\tvalid_0's rmse_kcatkm: 2.6314\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[3229]\tvalid_0's rmse_kcatkm: 2.6287\n",
      "valid_records min_index 3228\n",
      "[1000]\tvalid_0's rmse_kcatkm: 2.69186\n",
      "[2000]\tvalid_0's rmse_kcatkm: 2.64069\n",
      "[3000]\tvalid_0's rmse_kcatkm: 2.6314\n",
      "inner_predict 22338\n",
      "inner_predict 27921\n",
      "Val  {'logkm': (1.9856129846990465, 1.3673228738833758, 0.5714784450827153, 0.7670000462624499), 'logkcat': (2.0765796372561436, 1.4238455170584068, 0.6417719495752182, 0.8018218844837501), 'logkcatkm': (2.628700541386273, 1.9029471775335343, 0.6009403999561787, 0.7762359000837495)} \n",
      " Test {'logkm': (1.9346237793049645, 1.3526921923844013, 0.5834301106159893, 0.7739112499673839), 'logkcat': (2.1886327077453873, 1.4956231202435994, 0.6288223708178916, 0.7933749240279298), 'logkcatkm': (2.6974457678869537, 1.9344965827532108, 0.5727285115660933, 0.7572511985938475)}\n",
      "\n",
      "Fold 3\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[1000]\tvalid_0's rmse_kcatkm: 2.64816\n",
      "[2000]\tvalid_0's rmse_kcatkm: 2.59372\n",
      "[3000]\tvalid_0's rmse_kcatkm: 2.57861\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[3272]\tvalid_0's rmse_kcatkm: 2.5764\n",
      "valid_records min_index 3271\n",
      "[1000]\tvalid_0's rmse_kcatkm: 2.64816\n",
      "[2000]\tvalid_0's rmse_kcatkm: 2.59372\n",
      "[3000]\tvalid_0's rmse_kcatkm: 2.57861\n",
      "inner_predict 22335\n",
      "inner_predict 27921\n",
      "Val  {'logkm': (1.9436798367026524, 1.360730928672027, 0.5686716608862117, 0.76424189255595), 'logkcat': (2.1361342147328215, 1.470395651373148, 0.6306540330202627, 0.7948729541713193), 'logkcatkm': (2.576404344005892, 1.8525092929743974, 0.6070408183341651, 0.7794629569838863)} \n",
      " Test {'logkm': (1.9683509510707693, 1.3683343792335785, 0.5687790017407907, 0.7661282238114603), 'logkcat': (2.175354812471311, 1.4946611921886006, 0.6333123958475415, 0.7961054651414377), 'logkcatkm': (2.682047481872304, 1.9274172306491022, 0.5775927204736793, 0.7603388440095702)}\n",
      "\n",
      "Fold 4\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[1000]\tvalid_0's rmse_kcatkm: 2.73881\n",
      "[2000]\tvalid_0's rmse_kcatkm: 2.68726\n",
      "[3000]\tvalid_0's rmse_kcatkm: 2.66573\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[3237]\tvalid_0's rmse_kcatkm: 2.66189\n",
      "valid_records min_index 3236\n",
      "[1000]\tvalid_0's rmse_kcatkm: 2.73881\n",
      "[2000]\tvalid_0's rmse_kcatkm: 2.68726\n",
      "[3000]\tvalid_0's rmse_kcatkm: 2.66573\n",
      "inner_predict 22335\n",
      "inner_predict 27921\n",
      "Val  {'logkm': (1.876265994655511, 1.3290466682343216, 0.6040992140975245, 0.782824713280285), 'logkcat': (2.0288194454760062, 1.425930692392972, 0.6519358546276217, 0.8079895851008851), 'logkcatkm': (2.661887692970521, 1.9389805811690641, 0.5808289687998914, 0.7622350734330386)} \n",
      " Test {'logkm': (1.938124081340462, 1.3496546215193579, 0.5819213526536541, 0.7735116867667488), 'logkcat': (2.182033051422804, 1.5054138359984268, 0.6310575118896009, 0.7947041041443708), 'logkcatkm': (2.701462751753507, 1.943576235099876, 0.5714549952135347, 0.7563603216022672)}\n",
      "\n",
      "Fold 5\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[1000]\tvalid_0's rmse_kcatkm: 2.71134\n",
      "[2000]\tvalid_0's rmse_kcatkm: 2.66708\n",
      "[3000]\tvalid_0's rmse_kcatkm: 2.65658\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[3272]\tvalid_0's rmse_kcatkm: 2.65447\n",
      "valid_records min_index 3271\n",
      "[1000]\tvalid_0's rmse_kcatkm: 2.71134\n",
      "[2000]\tvalid_0's rmse_kcatkm: 2.66708\n",
      "[3000]\tvalid_0's rmse_kcatkm: 2.65658\n",
      "inner_predict 22335\n",
      "inner_predict 27921\n",
      "Val  {'logkm': (1.9128397967348785, 1.3539129962647654, 0.5824333023746291, 0.7745741130540493), 'logkcat': (2.114372757723646, 1.4545238359446009, 0.6482866613536592, 0.8061910439038178), 'logkcatkm': (2.6544709432115567, 1.9010345856824313, 0.5952241048030927, 0.7725769432770867)} \n",
      " Test {'logkm': (1.9057120032197963, 1.3449339444075268, 0.5957878434667836, 0.7810173770414853), 'logkcat': (2.1854046144354755, 1.4948198731878914, 0.6299164898868911, 0.7938129665578719), 'logkcatkm': (2.6876758738292743, 1.9483330474815719, 0.5758179805660795, 0.7589440470487053)}\n",
      "\n",
      "logkm\t RMSE\t MAE\t R2\t PCC\t\n",
      "Val  1.9259\t 1.3500\t 0.5822\t 0.7725\t\n",
      "Test 1.9381\t 1.3528\t 0.5819\t 0.7733\t\n",
      "logkcat\t RMSE\t MAE\t R2\t PCC\t\n",
      "Val  2.0949\t 1.4447\t 0.6407\t 0.8011\t\n",
      "Test 2.1885\t 1.4998\t 0.6289\t 0.7933\t\n",
      "logkcatkm\t RMSE\t MAE\t R2\t PCC\t\n",
      "Val  2.6506\t 1.9096\t 0.5930\t 0.7706\t\n",
      "Test 2.6988\t 1.9445\t 0.5723\t 0.7568\t\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import random\n",
    "import math\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "from scipy.stats import pearsonr\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "import lightgbmmt as lgb\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "\n",
    "def calculate_PWCBW_weights(values, high_value_threshold=None, high_value_weight=1.0, beta=0.9, decay_exponent=1/2):\n",
    "    task_weights = []\n",
    "    for name_idx, label_name in enumerate(task_names):\n",
    "        task_values = values[:, name_idx]\n",
    "        mask = task_values != fill_nan_value\n",
    "        filtered_values = task_values[mask]\n",
    "\n",
    "        def smooth_label(labels):\n",
    "            labels = labels - np.min(labels)\n",
    "            bin_index_per_label = (labels * 10).astype(int)\n",
    "\n",
    "            num_samples_of_bins = np.bincount(bin_index_per_label)\n",
    "\n",
    "            eff_label_dist = np.array([(1 - math.pow(beta, count)) / (1 - beta) for count in num_samples_of_bins])\n",
    "            eff_num_per_label = eff_label_dist[bin_index_per_label]\n",
    "            # eff_num_per_label = num_samples_of_bins[bin_index_per_label]\n",
    "\n",
    "            weights = (1 / (eff_num_per_label + 1e-6)) **  decay_exponent\n",
    "            return weights\n",
    "\n",
    "        weights = np.zeros_like(task_values)\n",
    "        weights[mask] = smooth_label(filtered_values)\n",
    "\n",
    "        if high_value_threshold is not None:\n",
    "            high_value_mask = task_values > high_value_threshold\n",
    "            weights[high_value_mask] *= high_value_weight\n",
    "        task_weights.append(weights)\n",
    "\n",
    "    weights_mean = np.array(task_weights).mean(axis=0)\n",
    "    return weights_mean\n",
    "\n",
    "\n",
    "def return_mtgbm_x_y(df_data, tasks):\n",
    "    y = np.array(df_data[tasks].values)\n",
    "\n",
    "    auxiliary_data = []\n",
    "    if use_t_ph_embedding:\n",
    "        ph = df_data['ph'].values.reshape(-1, 1)\n",
    "        t = df_data['t'].values.reshape(-1, 1)\n",
    "        auxiliary_data.append(ph)\n",
    "        auxiliary_data.append(t)\n",
    "\n",
    "    if use_mw_logp:\n",
    "        mw = df_data['mw'].values.reshape(-1, 1)\n",
    "        logp = df_data['logp'].values.reshape(-1, 1)\n",
    "        auxiliary_data.append(mw)\n",
    "        auxiliary_data.append(logp)\n",
    "\n",
    "    protein_data = np.array(df_data[protein_column].tolist())\n",
    "    substrate_data = np.array(df_data[substrate_column].tolist())\n",
    "\n",
    "    x = np.hstack([protein_data, substrate_data] + auxiliary_data)\n",
    "    return x, y\n",
    "\n",
    "def return_scores(y_true, y_pred):\n",
    "    mask = y_true != fill_nan_value\n",
    "    y_true = y_true[mask]\n",
    "    y_pred = y_pred[mask]\n",
    "\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    pcc = pearsonr(y_true, y_pred)[0]\n",
    "\n",
    "    return rmse, mae, r2, pcc\n",
    "\n",
    "def print_scores(task_scores_dict):\n",
    "    for task_name in task_names:\n",
    "        print(f\"{task_name}\\t RMSE\\t MAE\\t R2\\t PCC\\t\")\n",
    "\n",
    "        task_val_scores = task_scores_dict[task_name]['val']\n",
    "        task_test_scores = task_scores_dict[task_name]['test']\n",
    "\n",
    "        val_metrics = [f\"{np.mean(task_val_scores[metric_name]):.4f}\\t\" for metric_name in\n",
    "                       score_names]\n",
    "        print(\"Val  \" + \" \".join(val_metrics))\n",
    "\n",
    "        test_metrics = [f\"{np.mean(task_test_scores[metric_name]):.4f}\\t\" for metric_name in\n",
    "                        score_names]\n",
    "        print(\"Test \" + \" \".join(test_metrics))\n",
    "\n",
    "def self_kcatkm_rmse(preds, train_data):\n",
    "    labels = torch.tensor(train_data.get_label(), device=device)\n",
    "    preds = torch.tensor(preds, device=device)\n",
    "\n",
    "    # extract kcatkm values\n",
    "    labels = labels.view(num_tasks, -1).T[:, num_tasks - 1]\n",
    "    preds = preds.view(num_tasks, -1).T[:, num_tasks - 1]\n",
    "\n",
    "    # mask\n",
    "    valid_mask = labels != fill_nan_value\n",
    "    valid_labels = labels[valid_mask]\n",
    "    valid_preds = preds[valid_mask]\n",
    "\n",
    "    kcatkm_rmse = torch.sqrt(torch.mean((valid_labels - valid_preds) ** 2))\n",
    "\n",
    "    return 'rmse_kcatkm', kcatkm_rmse.item(), False\n",
    "\n",
    "def pcgrad(gradients):\n",
    "    adjusted_gradients = gradients.clone()\n",
    "\n",
    "    for i in range(num_tasks):\n",
    "        g_i = adjusted_gradients[:, i]\n",
    "        for j in range(num_tasks):\n",
    "            if i != j:  # 跳过与自身的比较\n",
    "                g_j = adjusted_gradients[:, j]\n",
    "\n",
    "                # 计算点积并检测冲突\n",
    "                dot_product = torch.dot(g_i, g_j)\n",
    "                if dot_product < 0:  # 仅在点积为负时调整\n",
    "                    projection = (dot_product / (torch.norm(g_j) ** 2 + 1e-8)) * g_j\n",
    "                    g_i -= projection  # 对 g_i 进行投影调整\n",
    "\n",
    "        # 更新调整后的梯度\n",
    "        adjusted_gradients[:, i] = g_i\n",
    "\n",
    "    return adjusted_gradients\n",
    "\n",
    "def cal_grad(preds, train_data, ep=0):\n",
    "    labels = torch.tensor(train_data.get_label(), device=device)\n",
    "    preds = torch.tensor(preds, device=device)\n",
    "    weights = torch.tensor(train_data.get_weight(), device=device)\n",
    "\n",
    "    labels = labels.view(num_tasks, -1).T\n",
    "    preds = preds.view(num_tasks, -1).T  # (num_samples, num_tasks)\n",
    "    weights = weights.unsqueeze(1).expand(-1, num_tasks)  # weights (num_samples,) -> (num_samples, num_tasks)\n",
    "\n",
    "    # mask\n",
    "    valid_mask = labels != fill_nan_value\n",
    "    grad = torch.zeros_like(preds)\n",
    "    grad[valid_mask] = preds[valid_mask] - labels[valid_mask]\n",
    "\n",
    "    # project\n",
    "    grad = pcgrad(grad)\n",
    "\n",
    "    # grad norm\n",
    "    grad_norms = torch.norm(grad, dim=0)\n",
    "    avg_grad_norm = grad_norms.mean()\n",
    "    grad_ratio = grad_norms / avg_grad_norm\n",
    "    grad_by_ratio = grad * grad_ratio\n",
    "    grad_final = grad_by_ratio.sum(dim=1)\n",
    "\n",
    "    # Hessian\n",
    "    grad_flattened = grad.T.flatten()\n",
    "    # hess = torch.ones_like(grad_final)\n",
    "    hess = torch.ones_like(grad_final) * weights.mean()  # hess with weights\n",
    "    hess2 = torch.ones_like(grad_flattened)\n",
    "\n",
    "    return grad_final.cpu().numpy(), hess.cpu().numpy(), grad_flattened.cpu().numpy(), hess2.cpu().numpy()\n",
    "\n",
    "# TODO Train model\n",
    "def train_mtgbm(params):\n",
    "    temp_params = deepcopy(params)\n",
    "    temp_params.update({\"verbosity\": -1, \"objective\": \"custom\", \"num_labels\": num_tasks, \"tree_learner\": 'serial2', \"num_threads\": num_threads})\n",
    "    num_iterations = temp_params.pop(\"num_iterations\")\n",
    "\n",
    "    task_scores_dict = {task_name: {'val': {name: [] for name in score_names}, 'test': {name: [] for name in score_names}} for task_name in task_names}\n",
    "\n",
    "    high_value_threshold = 5\n",
    "    high_value_weight = 3\n",
    "    beta, decay_exponent = 0.85, 0.46\n",
    "\n",
    "    for fold_idx, (train_index, val_index) in enumerate(kf.split(train_val_x), start=1):\n",
    "        print(f\"Fold {fold_idx}\")\n",
    "        # split dataset\n",
    "        train_x, val_x = train_val_x[train_index], train_val_x[val_index]\n",
    "        train_y, val_y = train_val_y[train_index], train_val_y[val_index]\n",
    "\n",
    "        train_data = lgb.Dataset(train_x, label=train_y, weight=calculate_PWCBW_weights(train_y, high_value_threshold, high_value_weight, beta, decay_exponent))\n",
    "        val_data = lgb.Dataset(val_x, label=val_y)\n",
    "\n",
    "        # get the best epoch number\n",
    "        evals_result_mt = {}\n",
    "        lgb.train(temp_params, train_data, num_iterations, valid_sets=[val_data],\n",
    "                  fobj=cal_grad, feval=self_kcatkm_rmse, verbose_eval=1000, evals_result=evals_result_mt,\n",
    "                  callbacks=[lgb.early_stopping(stopping_rounds=500)])\n",
    "        valid_records = evals_result_mt['valid_0']['rmse_kcatkm']\n",
    "        min_index = np.argmin(np.array(valid_records))\n",
    "        print(f\"valid_records min_index {min_index}\")\n",
    "\n",
    "        # train model for all scores of validation and test\n",
    "        train_data = lgb.Dataset(train_x, label=train_y, weight=calculate_PWCBW_weights(train_y, high_value_threshold, high_value_weight, beta, decay_exponent))\n",
    "        val_data = lgb.Dataset(val_x, label=val_y)\n",
    "        evals_result_mt = {}\n",
    "        model = lgb.train(temp_params, train_data, min_index + 1, valid_sets=[val_data],\n",
    "                          fobj=cal_grad, feval=self_kcatkm_rmse, verbose_eval=1000,\n",
    "                          evals_result=evals_result_mt)\n",
    "        model.set_num_labels(num_tasks)\n",
    "\n",
    "        # validation predict\n",
    "        val_predicted = model.predict(val_x)\n",
    "        val_scores = {task_name: return_scores(val_y[:, idx], val_predicted[:, idx]) for idx, task_name in enumerate(task_names)}\n",
    "\n",
    "        # test predict\n",
    "        test_predicted = model.predict(test_x)\n",
    "        test_scores = {task_name: return_scores(test_y[:, idx], test_predicted[:, idx]) for idx, task_name in enumerate(task_names)}\n",
    "\n",
    "        # record\n",
    "        for task_name in task_names:\n",
    "            for score_idx, score_name in enumerate(score_names):\n",
    "                task_scores_dict[task_name]['val'][score_name].append(val_scores[task_name][score_idx])\n",
    "                task_scores_dict[task_name]['test'][score_name].append(test_scores[task_name][score_idx])\n",
    "        print(f\"Val  {val_scores} \\n Test {test_scores}\\n\")\n",
    "\n",
    "    print_scores(task_scores_dict)\n",
    "\n",
    "\n",
    "# init seed\n",
    "random_state = 66\n",
    "random.seed(random_state)\n",
    "np.random.seed(random_state)\n",
    "torch.manual_seed(random_state)\n",
    "torch.cuda.manual_seed(random_state)\n",
    "torch.cuda.manual_seed_all(random_state)\n",
    "\n",
    "# config\n",
    "protein_column,  substrate_column = 'prott5', 'molebert'\n",
    "input_model = 'mtgbm_km_kcat_kcatkm'\n",
    "dataset_path = f'{current_dir}/../../data_process/dataset/df_all_log_transformed.pkl'\n",
    "params_json_path = f'{current_dir}/../{input_model}_params.json'\n",
    "use_t_ph_embedding = True\n",
    "use_mw_logp = True\n",
    "num_threads = 32\n",
    "search_max_evals = 60\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Current device is {device}\")\n",
    "\n",
    "# input\n",
    "score_names = ['rmse', 'mae', 'r2', 'pcc']\n",
    "task_names = ['logkm', 'logkcat', 'logkcatkm']\n",
    "num_tasks = len(task_names)\n",
    "df_input = pd.read_pickle(dataset_path)\n",
    "fill_nan_value = -100\n",
    "df_input = df_input.fillna(fill_nan_value)\n",
    "\n",
    "# split dataset\n",
    "df_train_val, df_test = train_test_split(df_input, test_size=0.2, random_state=random_state)\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=random_state)\n",
    "train_val_x, train_val_y = return_mtgbm_x_y(df_train_val, task_names)\n",
    "test_x, test_y = return_mtgbm_x_y(df_test, task_names)\n",
    "\n",
    "if os.path.exists(params_json_path):\n",
    "    with open(params_json_path, 'r') as json_file:\n",
    "        best_params = json.load(json_file)\n",
    "\n",
    "    print('best_params:', best_params)\n",
    "    print('using km kcat kcatkm resample')\n",
    "    train_mtgbm(best_params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "km_predict",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "391ae7a9a517ea640120dfb04776679e361e6af223196de029c77b4062e2a450"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
