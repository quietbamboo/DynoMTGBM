{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1725dc263c8399f4",
   "metadata": {},
   "source": [
    "## UniRep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from unirep.run_inference import BatchInference\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "# 过滤掉所有DeprecationWarning\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "df_all = pd.read_json('../dataset/df_multi_tasks.json')\n",
    "input_sequences = df_all['sequence'].drop_duplicates()\n",
    "print(f'number of sequences : {len(input_sequences)}')\n",
    "input_sequences.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fcfbe3b160a3125c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-24T08:23:31.817955Z",
     "start_time": "2024-12-24T06:47:40.014103Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ching\\anaconda3\\envs\\dynokp\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\ching\\Desktop\\DynoMTGBM_main\\data_preprocess\\inferred_functions\\unirep\\unirep.py:362: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\ching\\Desktop\\DynoMTGBM_main\\data_preprocess\\inferred_functions\\unirep\\unirep.py:29: Categorical.__init__ (from tensorflow.python.ops.distributions.categorical) is deprecated and will be removed after 2019-01-01.\n",
      "Instructions for updating:\n",
      "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n",
      "WARNING:tensorflow:From C:\\Users\\ching\\anaconda3\\envs\\dynokp\\lib\\site-packages\\tensorflow\\python\\ops\\distributions\\categorical.py:242: Distribution.__init__ (from tensorflow.python.ops.distributions.distribution) is deprecated and will be removed after 2019-01-01.\n",
      "Instructions for updating:\n",
      "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n",
      "WARNING:tensorflow:From C:\\Users\\ching\\anaconda3\\envs\\dynokp\\lib\\site-packages\\tensorflow\\python\\ops\\distributions\\categorical.py:278: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.random.categorical instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pandas bar: 100%|██████████| 8716/8716 [1:34:02<00:00,  1.54it/s]\n"
     ]
    }
   ],
   "source": [
    "protein_names, sequences = input_sequences.values, input_sequences.values\n",
    "\n",
    "# convert format\n",
    "seq_fasta_file_path = './unirep/tep.fasta'\n",
    "fasta_file = open(seq_fasta_file_path, \"w\")\n",
    "for name, seq in zip(protein_names, sequences):\n",
    "    fasta_file.write(\">\" + name + \"/n\" +seq + \"/n\")\n",
    "fasta_file.close()\n",
    "\n",
    "# calculate\n",
    "sess = tf.Session(config=config)\n",
    "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.8)  # 改变这个百分比即可\n",
    "sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\n",
    "inf_obj = BatchInference(batch_size=2)\n",
    "df_unirep_5400 = inf_obj.run_inference(filepath=seq_fasta_file_path)\n",
    "df_unirep_5400.to_csv('./unirep/df_unirep.tsv', sep='/t')\n",
    "\n",
    "# convert format\n",
    "df_unirep_1900 = pd.read_csv('./unirep/df_unirep.tsv', sep='/t').drop(\n",
    "    columns='Unnamed: 0').iloc[:, :1900]\n",
    "\n",
    "unirep_values = df_unirep_1900.values.tolist()\n",
    "\n",
    "unirep_col_value = []\n",
    "for idx_row in range(len(unirep_values)):\n",
    "    unirep_coupled = [rep_ for rep_ in unirep_values[idx_row]]\n",
    "    unirep_col_value.append(np.array(unirep_coupled))\n",
    "\n",
    "dict_result = {'sequences': sequences}\n",
    "dict_result[\"unirep\"] = unirep_col_value\n",
    "pd.DataFrame(dict_result).to_pickle('results/df_unirep.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e889a2a859315c",
   "metadata": {},
   "source": [
    "# ESM-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcfecf89cbc975ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sequence length < 2100 AAs, calculating with A800 with 80GB GPU memory\n",
    "# # ESM-2\n",
    "# import warnings\n",
    "# from esm2.run_calculate_esm2 import run_calculate\n",
    "# import pandas as pd\n",
    "# from tqdm import tqdm\n",
    "#\n",
    "# # 过滤掉所有DeprecationWarning\n",
    "# warnings.filterwarnings('ignore')\n",
    "#\n",
    "# df_all = pd.read_json('../ori_data/df_multi_tasks.json')\n",
    "# input_sequences = df_all['sequence'].drop_duplicates()\n",
    "# print(f'number of sequences : {len(input_sequences)}')\n",
    "#\n",
    "# protein_names, sequences = [], []\n",
    "# for count, value in enumerate(tqdm(input_sequences)):\n",
    "#     line=value.strip('/n')\n",
    "#     if count % 2 == 0:\n",
    "#         protein_names.append(line)\n",
    "#     else:\n",
    "#         if len(line) < 2100:\n",
    "#             sequences.append(line)\n",
    "#         else:\n",
    "#             protein_names = protein_names[:-1]\n",
    "#\n",
    "# print(f\"length of protein sequences: {len(sequences)}\")\n",
    "# df_result = pd.DataFrame({'sequences': sequences})\n",
    "#\n",
    "# seq_data = [(protein_name_, protein_sequence_) for protein_name_, protein_sequence_ in zip(protein_names, sequences)]\n",
    "#\n",
    "# esm2_1280 = run_calculate(seq_data, batch_size=1)\n",
    "# df_result['esm2'] = esm2_1280\n",
    "# df_result.to_pickle('/results/df_esm2.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "daf0a02d71f063f2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-24T18:54:40.855292Z",
     "start_time": "2024-12-24T18:54:40.751197Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16778\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sequences</th>\n",
       "      <th>esm2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MSIIKSYAAKEAGSELELYEYDAGELRPEDVEVQVDYCGICHSDLS...</td>\n",
       "      <td>[-0.050770897, -0.036682352, -0.018163353, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MDIDRLFSVKGMNAVVLGASSGIGKAIAEMFSEMGGKVVLSDIDEE...</td>\n",
       "      <td>[0.05781516, -0.055177405, 0.020610005, -0.031...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MRAVRLVEIGKPLSLQEIGVPKPKGPQVLIKVEAAGVCHSDVHMRQ...</td>\n",
       "      <td>[0.04404524, -0.054761235, -0.0029805414, 0.03...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MKAAVVREKNDGFVDLIDDWKPRELGFGDALVDVEYCGLCHTDLHC...</td>\n",
       "      <td>[-0.016896145, -0.05097463, -0.039859887, 0.03...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MKAAVVEQFKEPLKIKEVEKPTISYGEVLVRIKACGVCHTDLHAAH...</td>\n",
       "      <td>[0.024043683, -0.13023601, -0.09008518, 0.0502...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           sequences  \\\n",
       "0  MSIIKSYAAKEAGSELELYEYDAGELRPEDVEVQVDYCGICHSDLS...   \n",
       "1  MDIDRLFSVKGMNAVVLGASSGIGKAIAEMFSEMGGKVVLSDIDEE...   \n",
       "2  MRAVRLVEIGKPLSLQEIGVPKPKGPQVLIKVEAAGVCHSDVHMRQ...   \n",
       "3  MKAAVVREKNDGFVDLIDDWKPRELGFGDALVDVEYCGLCHTDLHC...   \n",
       "4  MKAAVVEQFKEPLKIKEVEKPTISYGEVLVRIKACGVCHTDLHAAH...   \n",
       "\n",
       "                                                esm2  \n",
       "0  [-0.050770897, -0.036682352, -0.018163353, 0.0...  \n",
       "1  [0.05781516, -0.055177405, 0.020610005, -0.031...  \n",
       "2  [0.04404524, -0.054761235, -0.0029805414, 0.03...  \n",
       "3  [-0.016896145, -0.05097463, -0.039859887, 0.03...  \n",
       "4  [0.024043683, -0.13023601, -0.09008518, 0.0502...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df_esm2 = pd.read_pickle('./results/df_esm2.pkl')\n",
    "print(len(df_esm2))\n",
    "df_esm2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72020e8648a56ada",
   "metadata": {},
   "source": [
    "# ESM-C using python 3.10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b45e780bc03312c2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-28T17:11:45.163308Z",
     "start_time": "2024-12-28T11:00:17.755359Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ching\\anaconda3\\envs\\esmc\\lib\\site-packages\\esm\\tokenization\\function_tokenizer.py:10: UserWarning: A NumPy version >=1.23.5 and <2.3.0 is required for this version of SciPy (detected version 1.21.5)\n",
      "  import scipy.sparse as sp\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fa55b5051014b748bd91a210e498c91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of sequences : 16813\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16813/16813 [6:11:16<00:00,  1.32s/it]   \n"
     ]
    }
   ],
   "source": [
    "from esm.models.esmc import ESMC\n",
    "from esm.sdk.api import (\n",
    "    ESMCInferenceClient,\n",
    "    ESMProtein,\n",
    "    LogitsConfig,\n",
    "    LogitsOutput,\n",
    ")\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "# 过滤掉所有DeprecationWarning\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "def main(client=ESMCInferenceClient, sequence=''):\n",
    "    # ================================================================\n",
    "    # Example usage: one single protein\n",
    "    # ================================================================\n",
    "    protein = ESMProtein(sequence=sequence)\n",
    "\n",
    "    # Use logits endpoint. Using bf16 for inference optimization\n",
    "    protein_tensor = client.encode(protein)\n",
    "    output = client.logits(\n",
    "        protein_tensor,\n",
    "        LogitsConfig(sequence=True, return_embeddings=True, return_hidden_states=True),\n",
    "    )\n",
    "    assert isinstance(\n",
    "        output, LogitsOutput\n",
    "    ), f\"LogitsOutput was expected but got {output}\"\n",
    "    assert output.logits is not None and output.logits.sequence is not None\n",
    "    assert output.embeddings is not None\n",
    "    assert output.hidden_states is not None\n",
    "\n",
    "\n",
    "    embeddings = output.embeddings\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "model = ESMC.from_pretrained(\"esmc_600m\")\n",
    "\n",
    "df_all = pd.read_json('../dataset/df_multi_tasks.json')\n",
    "input_sequences = df_all['sequence'].drop_duplicates()\n",
    "print(f'number of sequences : {len(input_sequences)}')\n",
    "\n",
    "esmc_embeddings = []\n",
    "for sequence in tqdm(input_sequences):\n",
    "    embeddings = main(client=model, sequence=sequence).numpy().mean(axis=1)[0, :]\n",
    "    esmc_embeddings.append(embeddings)\n",
    "\n",
    "dict_result = {'sequences': input_sequences, 'esmc': esmc_embeddings}\n",
    "pd.DataFrame(dict_result).to_json('results/df_esmc.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a4e81ac03d03045",
   "metadata": {},
   "source": [
    "## ProtT5- XL-Uniref50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fbc48fff2c6e9008",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-01-01T08:12:45.965180Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/myconda/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of sequences : 16813\n",
      "device:  cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8407/8407 [15:15<00:00,  9.18it/s]  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sequences</th>\n",
       "      <th>prott5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MSIIKSYAAKEAGSELELYEYDAGELRPEDVEVQVDYCGICHSDLS...</td>\n",
       "      <td>[0.016611475, 0.09350236, 0.009273678, 0.02968...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MDIDRLFSVKGMNAVVLGASSGIGKAIAEMFSEMGGKVVLSDIDEE...</td>\n",
       "      <td>[0.06827993, -0.029304748, 0.03184763, -0.0951...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MRAVRLVEIGKPLSLQEIGVPKPKGPQVLIKVEAAGVCHSDVHMRQ...</td>\n",
       "      <td>[0.036032353, 0.019176442, 0.020855611, -0.072...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>MKAAVVREKNDGFVDLIDDWKPRELGFGDALVDVEYCGLCHTDLHC...</td>\n",
       "      <td>[-0.0021131546, 0.005914332, -0.01885727, -5.2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>MKAAVVEQFKEPLKIKEVEKPTISYGEVLVRIKACGVCHTDLHAAH...</td>\n",
       "      <td>[0.032064147, 0.08067483, 0.024847964, -0.0083...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sequences  \\\n",
       "0   MSIIKSYAAKEAGSELELYEYDAGELRPEDVEVQVDYCGICHSDLS...   \n",
       "2   MDIDRLFSVKGMNAVVLGASSGIGKAIAEMFSEMGGKVVLSDIDEE...   \n",
       "4   MRAVRLVEIGKPLSLQEIGVPKPKGPQVLIKVEAAGVCHSDVHMRQ...   \n",
       "17  MKAAVVREKNDGFVDLIDDWKPRELGFGDALVDVEYCGLCHTDLHC...   \n",
       "18  MKAAVVEQFKEPLKIKEVEKPTISYGEVLVRIKACGVCHTDLHAAH...   \n",
       "\n",
       "                                               prott5  \n",
       "0   [0.016611475, 0.09350236, 0.009273678, 0.02968...  \n",
       "2   [0.06827993, -0.029304748, 0.03184763, -0.0951...  \n",
       "4   [0.036032353, 0.019176442, 0.020855611, -0.072...  \n",
       "17  [-0.0021131546, 0.005914332, -0.01885727, -5.2...  \n",
       "18  [0.032064147, 0.08067483, 0.024847964, -0.0083...  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5EncoderModel\n",
    "import torch\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, sys\n",
    "current_dir = os.path.dirname(sys.argv[0])\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "def run_calculate(sequences, tokenizer, model, batch_size=2):\n",
    "    embeddings = []\n",
    "    error_sequences = []\n",
    "    lengths = [len(seq) for seq in sequences]\n",
    "\n",
    "    # replace all rare/ambiguous amino acids by X and introduce white-space between all amino acids\n",
    "    sequence_preprocess = [\" \".join(list(re.sub(r\"[UZOB]\", \"X\", sequence))) for sequence in sequences]\n",
    "\n",
    "    # batch\n",
    "    for start_idx in trange(0, len(sequence_preprocess), batch_size):\n",
    "        batch_sequences = sequence_preprocess[start_idx: start_idx + batch_size]\n",
    "        length_of_sequences = lengths[start_idx: start_idx + batch_size]\n",
    "\n",
    "        # tokenize sequences and pad up to the longest sequence in the batch\n",
    "        ids = tokenizer(batch_sequences, add_special_tokens=True, padding=\"longest\")\n",
    "\n",
    "        input_ids = torch.tensor(ids['input_ids']).to(device)\n",
    "        attention_mask = torch.tensor(ids['attention_mask']).to(device)\n",
    "\n",
    "        try:\n",
    "            # generate embeddings\n",
    "            with torch.no_grad():\n",
    "                embedding_repr = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "            # extract residue embeddings for the sequences in the batch and remove padded & special tokens\n",
    "            for idx_in_batch in range(len(batch_sequences)):\n",
    "                embedding = embedding_repr.last_hidden_state[idx_in_batch, :length_of_sequences[idx_in_batch]].mean(dim=0)\n",
    "                embeddings.append(embedding.cpu().numpy())\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing batch starting at index {start_idx}: {e}\")\n",
    "            error_sequences.extend(batch_sequences)\n",
    "\n",
    "    df_result = pd.DataFrame({'sequences': sequences, 'prott5': embeddings})\n",
    "    return df_result\n",
    "\n",
    "# config\n",
    "df_all = pd.read_json('../dataset/df_multi_tasks.json')\n",
    "input_sequences = df_all['sequence'].drop_duplicates()\n",
    "print(f'number of sequences : {len(input_sequences)}')\n",
    "\n",
    "pre_models_path = './prott5/pre_models'\n",
    "\n",
    "# model\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print('device: ', device)\n",
    "# Load the tokenizer\n",
    "tokenizer = T5Tokenizer.from_pretrained(pre_models_path, do_lower_case=False)\n",
    "# Load the model\n",
    "model = T5EncoderModel.from_pretrained(pre_models_path).to(device)\n",
    "\n",
    "# calculate\n",
    "df_result = run_calculate(input_sequences, tokenizer, model, batch_size=2)\n",
    "df_result.to_json('./results/df_protT5.json')\n",
    "df_result.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb59c0149797638",
   "metadata": {},
   "source": [
    "# Prost-T5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96a3ec928985e9a8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-02T12:41:29.275825Z",
     "start_time": "2025-01-02T12:40:28.118912Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/myconda/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of sequences : 16813\n",
      "device:  cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16813/16813 [14:29<00:00, 19.34it/s] \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sequences</th>\n",
       "      <th>prost5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MSIIKSYAAKEAGSELELYEYDAGELRPEDVEVQVDYCGICHSDLS...</td>\n",
       "      <td>[0.0017113135, -0.030387186, -0.0036463677, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MDIDRLFSVKGMNAVVLGASSGIGKAIAEMFSEMGGKVVLSDIDEE...</td>\n",
       "      <td>[-0.0041703084, -0.047611132, -0.0069104224, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MRAVRLVEIGKPLSLQEIGVPKPKGPQVLIKVEAAGVCHSDVHMRQ...</td>\n",
       "      <td>[0.014685851, -0.013136651, 0.0020611903, -0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>MKAAVVREKNDGFVDLIDDWKPRELGFGDALVDVEYCGLCHTDLHC...</td>\n",
       "      <td>[0.006702779, -0.02026054, 0.007960504, -0.008...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>MKAAVVEQFKEPLKIKEVEKPTISYGEVLVRIKACGVCHTDLHAAH...</td>\n",
       "      <td>[0.030726543, -0.020222707, -0.009790693, 0.02...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sequences  \\\n",
       "0   MSIIKSYAAKEAGSELELYEYDAGELRPEDVEVQVDYCGICHSDLS...   \n",
       "2   MDIDRLFSVKGMNAVVLGASSGIGKAIAEMFSEMGGKVVLSDIDEE...   \n",
       "4   MRAVRLVEIGKPLSLQEIGVPKPKGPQVLIKVEAAGVCHSDVHMRQ...   \n",
       "17  MKAAVVREKNDGFVDLIDDWKPRELGFGDALVDVEYCGLCHTDLHC...   \n",
       "18  MKAAVVEQFKEPLKIKEVEKPTISYGEVLVRIKACGVCHTDLHAAH...   \n",
       "\n",
       "                                               prost5  \n",
       "0   [0.0017113135, -0.030387186, -0.0036463677, 0....  \n",
       "2   [-0.0041703084, -0.047611132, -0.0069104224, -...  \n",
       "4   [0.014685851, -0.013136651, 0.0020611903, -0.0...  \n",
       "17  [0.006702779, -0.02026054, 0.007960504, -0.008...  \n",
       "18  [0.030726543, -0.020222707, -0.009790693, 0.02...  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5EncoderModel\n",
    "import torch\n",
    "import pandas as pd\n",
    "import re\n",
    "from tqdm import trange\n",
    "\n",
    "def run_calculate(sequences, tokenizer, model, batch_size=2):\n",
    "    embeddings = []\n",
    "    valid_sequences = []\n",
    "    error_sequences = []\n",
    "    lengths = [len(seq) for seq in sequences]\n",
    "\n",
    "    sequences = [\" \".join(list(re.sub(r\"[UZOB]\", \"X\", sequence))) for sequence in sequences]\n",
    "    sequences = [ \"<AA2fold>\" + \" \" + s if s.isupper() else \"<fold2AA>\" + \" \" + s  for s in sequences]\n",
    "\n",
    "    # batch\n",
    "    for start_idx in trange(0, len(sequences), batch_size):\n",
    "        batch_sequences = sequences[start_idx: start_idx + batch_size]\n",
    "        length_of_sequences = lengths[start_idx: start_idx + batch_size]\n",
    "\n",
    "        # tokenize sequences and pad up to the longest sequence in the batch\n",
    "        ids = tokenizer.batch_encode_plus(batch_sequences, add_special_tokens=True, padding=\"longest\",return_tensors='pt').to(device)\n",
    "\n",
    "        try:\n",
    "            # generate embeddings\n",
    "            with torch.no_grad():\n",
    "                embedding_rpr = model(ids.input_ids, attention_mask=ids.attention_mask)\n",
    "\n",
    "                # extract residue embeddings for the sequences in the batch and remove padded & special tokens\n",
    "                for idx_in_batch in range(len(batch_sequences)):\n",
    "                    embedding = embedding_rpr.last_hidden_state[idx_in_batch, :length_of_sequences[idx_in_batch]].mean(dim=0)\n",
    "                    embeddings.append(embedding.cpu().numpy())\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing batch starting at index {start_idx}: {e}\")\n",
    "            error_sequences.extend(batch_sequences)\n",
    "\n",
    "    df_result = pd.DataFrame({'sequences': input_sequences, 'prost5': embeddings})\n",
    "    return df_result\n",
    "\n",
    "\n",
    "# config\n",
    "df_all = pd.read_json('../dataset/df_multi_tasks.json')\n",
    "input_sequences = df_all['sequence'].drop_duplicates()\n",
    "print(f'number of sequences : {len(input_sequences)}')\n",
    "\n",
    "# model\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print('device: ', device)\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = T5Tokenizer.from_pretrained('./prostt5', do_lower_case=False)\n",
    "model = T5EncoderModel.from_pretrained(\"./prostt5\").to(device)\n",
    "\n",
    "# calculate\n",
    "df_result = run_calculate(input_sequences, tokenizer, model, batch_size=1)\n",
    "df_result.to_json('./results/df_prostT5.json')\n",
    "df_result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4958d201-b71e-4948-a809-50ad5fac148b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myconda",
   "language": "python",
   "name": "myconda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
